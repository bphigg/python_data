{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4398aa01-7392-4436-87ad-0abe12ff2454",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b232b8-e823-4358-bb10-e8efbe43b9c8",
   "metadata": {},
   "source": [
    "For this assignment, we are going to calculate summaries from the **NFL Scores** dataset. First, we will calculate the summaries utilizing python's ```map/reduce``` functionality, then we will calculate the summaries using ```pyspark```'s *spark SQL* and *pandas-on-spark*.\n",
    "\n",
    "Let's start by reading in some libraries that we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "573ae936-cbda-489c-b761-aa7d5924b0cf",
   "metadata": {
    "id": "573ae936-cbda-489c-b761-aa7d5924b0cf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import round\n",
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c81e81-2c3c-4a3e-a64c-9a931f68870d",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e0a53b-5a7e-4971-b9aa-7c0708aa6c4d",
   "metadata": {},
   "source": [
    "To utilize python's ```map/reduce``` functionality, we will first need to split the data set into separate 'chunks.' So let's read in the data as a ```pandas``` data frame and take a look at the first few rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e6004e30-afaa-4d0f-ae2b-02100f2eef5a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "e6004e30-afaa-4d0f-ae2b-02100f2eef5a",
    "outputId": "995c7f8e-0972-4b0e-d1eb-831c07db133a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week</th>\n",
       "      <th>date</th>\n",
       "      <th>day</th>\n",
       "      <th>season</th>\n",
       "      <th>awayTeam</th>\n",
       "      <th>AQ1</th>\n",
       "      <th>AQ2</th>\n",
       "      <th>AQ3</th>\n",
       "      <th>AQ4</th>\n",
       "      <th>AOT</th>\n",
       "      <th>...</th>\n",
       "      <th>homeFumLost</th>\n",
       "      <th>homeNumPen</th>\n",
       "      <th>homePenYds</th>\n",
       "      <th>home3rdConv</th>\n",
       "      <th>home3rdAtt</th>\n",
       "      <th>home4thConv</th>\n",
       "      <th>home4thAtt</th>\n",
       "      <th>homeTOP</th>\n",
       "      <th>HminusAScore</th>\n",
       "      <th>homeSpread</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5-Sep</td>\n",
       "      <td>Thu</td>\n",
       "      <td>2002</td>\n",
       "      <td>San Francisco 49ers</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>80</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32.47</td>\n",
       "      <td>-3</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8-Sep</td>\n",
       "      <td>Sun</td>\n",
       "      <td>2002</td>\n",
       "      <td>Minnesota Vikings</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.48</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8-Sep</td>\n",
       "      <td>Sun</td>\n",
       "      <td>2002</td>\n",
       "      <td>New Orleans Saints</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>31.48</td>\n",
       "      <td>-6</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>8-Sep</td>\n",
       "      <td>Sun</td>\n",
       "      <td>2002</td>\n",
       "      <td>New York Jets</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>82</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>39.13</td>\n",
       "      <td>-6</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>8-Sep</td>\n",
       "      <td>Sun</td>\n",
       "      <td>2002</td>\n",
       "      <td>Arizona Cardinals</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>56</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>34.40</td>\n",
       "      <td>8</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  week   date  day  season             awayTeam  AQ1  AQ2  AQ3  AQ4  AOT  ...  \\\n",
       "0    1  5-Sep  Thu    2002  San Francisco 49ers    3    0    7    6   -1  ...   \n",
       "1    1  8-Sep  Sun    2002    Minnesota Vikings    3   17    0    3   -1  ...   \n",
       "2    1  8-Sep  Sun    2002   New Orleans Saints    6    7    7    0    6  ...   \n",
       "3    1  8-Sep  Sun    2002        New York Jets    0   17    3   11    6  ...   \n",
       "4    1  8-Sep  Sun    2002    Arizona Cardinals   10    3    3    7   -1  ...   \n",
       "\n",
       "   homeFumLost  homeNumPen homePenYds  home3rdConv  home3rdAtt  home4thConv  \\\n",
       "0            0          10         80            4           8            0   \n",
       "1            1           4         33            2           6            0   \n",
       "2            0           8         85            1           6            0   \n",
       "3            1          10         82            4           8            2   \n",
       "4            0           7         56            6          10            1   \n",
       "\n",
       "   home4thAtt  homeTOP  HminusAScore  homeSpread  \n",
       "0           1    32.47            -3        -4.0  \n",
       "1           0    28.48             4         4.5  \n",
       "2           1    31.48            -6         6.0  \n",
       "3           2    39.13            -6        -3.0  \n",
       "4           2    34.40             8         6.0  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfl_data = pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/scoresFull.csv\")\n",
    "nfl_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb055743-522e-41a2-bee6-a028e33ed5eb",
   "metadata": {},
   "source": [
    "For this exercise, let's go ahead split the datasets 'chunks' according to ```season```, select **HQ4** (Home Team 4th Quarter Points) as the variable to summarize and group the results according to the **date**. Since the date contains both the month and day of the game, we'll make the grouping a little easier and eliminate the day so that we are grouping on **month** only.\n",
    "\n",
    "The following ```for loop``` removes the day from the date column. We confirm the loop ran correctly by identifying the unique values in the date column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "jBXVZwY1Jp4D",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jBXVZwY1Jp4D",
    "outputId": "66c27678-ac6e-4d9c-9e8a-8b261e1ed156"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Dec', 'Feb', 'Jan', 'Nov', 'Oct', 'Sep'], dtype=object)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, row in nfl_data.iterrows():\n",
    "  month = row['date'][-3:]\n",
    "  nfl_data.at[index, 'date'] = month\n",
    "\n",
    "np.unique(np.array(nfl_data['date']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787adf06-a4ad-47c2-a900-342c70c249c2",
   "metadata": {},
   "source": [
    "Let's use the same ```np.unique()``` code to identify all the different seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a3c28a65-b521-46ed-b9c8-ad15c11c11d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3c28a65-b521-46ed-b9c8-ad15c11c11d9",
    "outputId": "a4798ad1-425a-4ff5-caf2-3e1f704c2bfb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012,\n",
       "       2013, 2014])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.array(nfl_data['season']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0507751e-3b0b-415f-b384-202216d1d79d",
   "metadata": {},
   "source": [
    "Now we'll split the data into separate *csv* files using a ```for loop```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2766f240-c538-4330-b33e-c2c5194d3139",
   "metadata": {
    "id": "2766f240-c538-4330-b33e-c2c5194d3139"
   },
   "outputs": [],
   "source": [
    "for i in range(2002, 2015):\n",
    "    nfl_data.loc[nfl_data[\"season\"] == i].to_csv('nfl'+ str(i) +'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7939c4e1-8345-40dd-a046-1643c0c98249",
   "metadata": {},
   "source": [
    "Lastly, we'll read the *csv* files back into a list, so that our end result will be a list containing the thirteen separate data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d714308a-81e2-4013-91f9-4bb65e1f74a4",
   "metadata": {
    "id": "d714308a-81e2-4013-91f9-4bb65e1f74a4"
   },
   "outputs": [],
   "source": [
    "nfl_sets = []\n",
    "for i in range(2002, 2015):\n",
    "    year = pd.read_csv('nfl'+str(i)+'.csv')\n",
    "    nfl_sets.append(year)\n",
    "#a = pd.read_csv('nfl2002.csv')\n",
    "#nfl_sets.append()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e382d2fc-696e-4a92-983c-f31e7bacc869",
   "metadata": {},
   "source": [
    "We'll utilize the ```len()``` function to confirm that our list contains the data sets.\n",
    "\n",
    "(We can also run ```nfl_sets[0]``` to check that the first index of the list is our first data set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "eae70ee1-0e68-4724-af66-7a61dc872136",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eae70ee1-0e68-4724-af66-7a61dc872136",
    "outputId": "2802a49b-ccca-476f-b66c-3e21bb332593"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nfl_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e202a3e9-e770-4594-a61b-fcbcf870d207",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6518d05-5b49-4fb1-afc4-5bb396cf25a5",
   "metadata": {},
   "source": [
    "For the **HQ4** variable, we are going to want to sum it across the grouping variable (**date**), sqaure it and sum that result across the grouping variable, and lastly, count it across the grouping variable. To do this, we'll want to create a dictionary with the *key* equal to the grouping variable and the *values* equal to a list containing the three measures (sum, sum of squared, count).\n",
    "\n",
    "The following function will take in a data set, grouping variable, and summary variable. It creates an empty dictionary, and then it will iterate over a data frame utilizing a ```for loop```. It will iterate through each row of the data frame first looking to see if the grouping variable exists in the dictionary ```group_dict```. If it does, then it will add the variable for that row to the existing sum, square the variable for that row and add it to an existing sum, and add one more to the count measure. If the grouping variable does not exist, then it will create a *key* for the group variable and populate the *values* with the variable value, variable squared, and count of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "NZ0oeItqSNZ9",
   "metadata": {
    "id": "NZ0oeItqSNZ9"
   },
   "outputs": [],
   "source": [
    "def map_vars(data, group, var):\n",
    "  group_dict = {}\n",
    "  for index, row in data.iterrows():\n",
    "    if row[group] in group_dict:\n",
    "      group_dict[row[group]] = [(group_dict[row[group]][0] + row[var]), (group_dict[row[group]][1] + (row[var]**2)), (group_dict[row[group]][2] + 1)]\n",
    "    else:\n",
    "      group_dict[row[group]] = [row[var], row[var]**2, 1]\n",
    "  return group_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df997c7-02a6-43e2-8521-7bc9a8e9d6b6",
   "metadata": {},
   "source": [
    "Let's run a quick test on one of our data sets to see that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "wGebXYvUXG4b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wGebXYvUXG4b",
    "outputId": "e53b3f67-cee4-4ad5-87d2-04aef09da3d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sep': [391, 4129, 60],\n",
       " 'Oct': [373, 4293, 56],\n",
       " 'Nov': [300, 2582, 62],\n",
       " 'Dec': [557, 6347, 78],\n",
       " 'Jan': [95, 1389, 11]}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_vars(nfl_sets[0], 'date', 'HQ4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c27298-fa50-48c5-b26e-8773662187ef",
   "metadata": {},
   "source": [
    "That's great! Our result is exactly what we want - a dictionary with *key*'s  as the grouping variable and *values* as a list of *sum, sum of squares, and count*.\n",
    "\n",
    "Now we're going to want to ```map``` this function to each of our thirteen data sets. The ```map``` function will take in a function as an argument as well a list of data sets the function will be applied to. But we will also stipulate the ```group``` and ```var``` variables for each iteration of the function. So we will create a list of the grouping and summary variables that are repeated and of the same length as our list of data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "xZ33R84gfTYc",
   "metadata": {
    "id": "xZ33R84gfTYc"
   },
   "outputs": [],
   "source": [
    "arg2=['date']*13\n",
    "arg3=['HQ4']*13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f95cc-fc0a-48ec-9191-d2de5ca1e5d2",
   "metadata": {},
   "source": [
    "Since the ```map``` function creates a mapping object, we'll utilize ```list()``` to create a list of our mapping results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "wkph_Y_rqfqq",
   "metadata": {
    "id": "wkph_Y_rqfqq"
   },
   "outputs": [],
   "source": [
    "mapped = list(map(map_vars, nfl_sets, arg2, arg3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bef908-95b2-4be9-835e-7f71e092fb3f",
   "metadata": {},
   "source": [
    "Let's access a few indices of our list to confirm that our mapping was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "qK5YbdGku7BX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qK5YbdGku7BX",
    "outputId": "383d2ef3-6d7d-4c83-e881-3ad40a888a47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sep': [391, 4129, 60],\n",
       " 'Oct': [373, 4293, 56],\n",
       " 'Nov': [300, 2582, 62],\n",
       " 'Dec': [557, 6347, 78],\n",
       " 'Jan': [95, 1389, 11]}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "pYskASWsXNgr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pYskASWsXNgr",
    "outputId": "91c70983-5181-4633-f4f6-82dc3e824dc5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sep': [422, 4506, 60],\n",
       " 'Oct': [291, 3029, 56],\n",
       " 'Nov': [499, 5425, 75],\n",
       " 'Dec': [389, 4227, 65],\n",
       " 'Jan': [61, 569, 10],\n",
       " 'Feb': [19, 361, 1]}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229dcb26-8bdb-4364-bbe1-b6ce596fb4cf",
   "metadata": {},
   "source": [
    "Great! It looks like our mapping function was a success. We now have a list of thirteen dictionaries, each with grouping results for each season or data 'chunk.'\n",
    "\n",
    "Now we need to combine our separate dictionaries into one dictionary. Let's start by writing a function that will combine two separate dictionaries into a single dictionary. Below, we have the function take in two dictionaries and start by creating an empty dictionay. We then iterate through the keys of the first dictionary utilizing a ```for loop``` combined with ```if/else``` logic: if the *key* in the first dictionary is also in the second dictionary, then we add that *key* to the empty dictionary and combine (add) the *values* of the key; if the *key* value is not in the second dictionary, then we simply copy the *key* and *values* of the first dictionary to the empty dictionary; lastly, we check if there were any *keys* in the second dictionary that were not in the first and copy those *key* and *values* to the (previously) empty dictionary. We return the new, combined dictionary at the end of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "Qtrn4mq5Ul1F",
   "metadata": {
    "id": "Qtrn4mq5Ul1F"
   },
   "outputs": [],
   "source": [
    "def reduce_vars(dict1, dict2):\n",
    "  combined = {}\n",
    "  for key in dict1:\n",
    "    if key in dict2:\n",
    "      combined[key] = [dict1[key][0]+dict2[key][0], dict1[key][1]+dict2[key][1], dict1[key][2]+dict2[key][2]]\n",
    "    else:\n",
    "      combined[key] = dict1[key]\n",
    "  for key in dict2:\n",
    "    if key not in dict1:\n",
    "      combined[key] = dict2[key]\n",
    "  return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4205054-47c2-437e-8b9c-21a8869ae13f",
   "metadata": {},
   "source": [
    "Let's run this function on two of our data frames contained in the ```list``` **mapped**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "n2HF3myVWDXp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n2HF3myVWDXp",
    "outputId": "e9ffbb2f-a15f-4881-982a-2283f0b9788c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sep': [813, 8635, 120],\n",
       " 'Oct': [664, 7322, 112],\n",
       " 'Nov': [799, 8007, 137],\n",
       " 'Dec': [946, 10574, 143],\n",
       " 'Jan': [156, 1958, 21],\n",
       " 'Feb': [19, 361, 1]}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce_vars(mapped[0], mapped[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64001d3-d442-4c80-868c-472ecb7b8419",
   "metadata": {},
   "source": [
    "We can look back on the contents of ```mapped[0]``` and ```mapped[1]``` and see that they were successfully combined!\n",
    "\n",
    "Now we will utilize ```reduce()``` from the ```functools``` library to apply our ```reduce_vars()``` function across all the dictionaries in the ```list``` **mapped**. ```reduce()``` will repeat the ```reduce_vars``` function, combining the first two data frames and then adding/combining each successive data frame to the 'running total' dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3xGhApTkXtPY",
   "metadata": {
    "id": "3xGhApTkXtPY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sep': [4456, 46934, 709],\n",
       " 'Oct': [5204, 58742, 796],\n",
       " 'Nov': [5307, 58377, 836],\n",
       " 'Dec': [5513, 60091, 909],\n",
       " 'Jan': [1378, 15314, 209],\n",
       " 'Feb': [89, 1147, 12]}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = functools.reduce(reduce_vars, mapped)\n",
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0632238a-73a2-4a41-8b98-7dbb55065ae9",
   "metadata": {},
   "source": [
    "Now let's transform these values into something more statistically interpretable - **average** and **standard deviation**\n",
    "\n",
    "We can utilize a ```for loop``` to iterate through each *key* in our dictionary and create the **mean** and **standard deviation** from the values in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "Ek1tkdDeZSuG",
   "metadata": {
    "id": "Ek1tkdDeZSuG"
   },
   "outputs": [],
   "source": [
    "def summation(dict_):\n",
    "  for key in dict_:\n",
    "    sum = dict_[key][0]\n",
    "    sumsqrd = dict_[key][1]\n",
    "    count = dict_[key][2]\n",
    "    mean = sum/count\n",
    "    dict_[key] = [round(mean, 2), round((np.sqrt((sumsqrd - (count*(mean**2)))/(count-1))), 2)]\n",
    "  return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "FT3M46Geabrb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FT3M46Geabrb",
    "outputId": "5b7b0246-f629-4d53-ed84-34624a2b8822"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sep': [6.28, 5.17],\n",
       " 'Oct': [6.54, 5.58],\n",
       " 'Nov': [6.35, 5.44],\n",
       " 'Dec': [6.06, 5.42],\n",
       " 'Jan': [6.59, 5.47],\n",
       " 'Feb': [7.42, 6.65]}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summation(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26959560-65fa-4693-96f5-1fc4da02c560",
   "metadata": {},
   "source": [
    "Looks great!\n",
    "\n",
    "Lastly, let's go ahead and write a wrapper function that will take in our list of data frames and *any* grouping variable and *any* summary variable and return the **mean** and **standard deviation** of the summary variable across the grouping variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "MyrtQvpTizda",
   "metadata": {
    "id": "MyrtQvpTizda"
   },
   "outputs": [],
   "source": [
    "def wrap_func(data, group, var):\n",
    "  arg2 = [group]*len(data)\n",
    "  arg3 = [var]*len(data)\n",
    "  maps = list(map(map_vars, data, arg2, arg3))\n",
    "  maps_reduce = functools.reduce(reduce_vars, maps)\n",
    "  result = summation(maps_reduce)\n",
    "  return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25615c2-72e3-4c61-b742-ccc952a2356f",
   "metadata": {},
   "source": [
    "Let's see how it does on a different grouping and summary variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "jhMwK_DmkMYu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jhMwK_DmkMYu",
    "outputId": "48be9f9c-545c-4ce6-9b47-fa3bc0219946"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': [19.72, 8.98],\n",
       " '2': [19.55, 10.43],\n",
       " '3': [20.77, 9.63],\n",
       " '4': [20.98, 10.06],\n",
       " '5': [20.53, 10.3],\n",
       " '6': [20.68, 10.27],\n",
       " '7': [21.11, 10.67],\n",
       " '8': [20.15, 9.78],\n",
       " '9': [22.15, 9.7],\n",
       " '10': [21.54, 10.02],\n",
       " '11': [19.59, 10.02],\n",
       " '12': [21.51, 10.8],\n",
       " '13': [20.44, 10.13],\n",
       " '14': [19.39, 10.2],\n",
       " '15': [20.94, 11.22],\n",
       " '16': [20.8, 10.33],\n",
       " '17': [19.24, 10.64],\n",
       " 'WildCard': [21.33, 10.13],\n",
       " 'Division': [21.0, 9.72],\n",
       " 'ConfChamp': [20.88, 8.14],\n",
       " 'SuperBowl': [27.62, 10.11]}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrap_func(nfl_sets, 'week', 'AFinal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3002cf83-822e-4555-9af9-7d9c66683f22",
   "metadata": {},
   "source": [
    "Fantastic!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5a4e4a-3bfe-43bd-8b6b-c5ac7b5a69e0",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70324dbb-e81f-4a71-bd3f-dabfb9682943",
   "metadata": {},
   "source": [
    "Now let's try getting some summary stats from the same data set using 'spark SQl' from ```pyspark```.\n",
    "\n",
    "We'll get the same summary stats as above, **mean** and **standard deviation**, but instead of one summary variable, we'll get the stats for **ALL** the point variables ('AQ1', 'AQ2', 'AQ3', 'AQ4', 'AFinal', 'HQ1', 'HQ2', 'HQ3', 'HQ4', 'HFinal').\n",
    "\n",
    "Let's start by setting up a ```SparkSession```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "79a3de02-e6f2-43d7-a231-38a8a09e778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[*]').appName('my_app').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38ed547-0aaf-4c17-bd32-7838b4b086be",
   "metadata": {},
   "source": [
    "Next, we'll create a *spark* data frame from our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "56c3e43f-33a2-4921-b870-22f8af2ae740",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_sp = spark.createDataFrame(nfl_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7e6117-c883-440f-aa9a-540a0172c99e",
   "metadata": {},
   "source": [
    "We can check to make sure the dataframe loaded correctly by inpsecting the schema and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "07dd1b1a-4568-4f62-99a2-b3e3fdd0ad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nfl_sp.printSchema() # to save space, I won't execute the cell; the data frame loaded correctly.\n",
    "# nfl_sp.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5ce007-238d-4f0b-8090-5deb0259e451",
   "metadata": {},
   "source": [
    "The great thing about 'spark SQL' is that we can utilize SQL coding directly. Here, we'll select all of our summary variables and perform the mean and standard deviation calculations through the ```.agg()``` function.  I've added some additional code that will round the results and add concise column headers so the results can be more easily read in the print out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cad8c0cb-bbc9-45b6-80f7-cf556f931ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:==================================>                   (81 + 47) / 128]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+\n",
      "|meanA1|stdA1|meanA2|stdA2|meanA3|stdA3|meanA4|stdA4|meanAF|stdAF|meanH1|stdH1|meanH2|stdH2|meanH3|stdH3|meanH4|stdH4|meanHF|stdHF|\n",
      "+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+\n",
      "|  3.92| 4.49|  6.24| 5.22|  4.39| 4.63|  5.89| 5.28| 20.56| 10.2|  4.83| 4.73|  7.11|  5.7|  4.79| 4.76|  6.32| 5.42| 23.17|10.41|\n",
      "+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "nfl_sp.select(['AQ1', 'AQ2', 'AQ3', 'AQ4', 'AFinal', 'HQ1', 'HQ2', 'HQ3', 'HQ4', 'HFinal']) \\\n",
    ".agg(round(avg('AQ1'),2).alias('meanA1'), round(std('AQ1'),2).alias('stdA1'), \\\n",
    "     round(avg('AQ2'),2).alias('meanA2'), round(std('AQ2'),2).alias('stdA2'), \\\n",
    "     round(avg('AQ3'),2).alias('meanA3'), round(std('AQ3'),2).alias('stdA3'), \\\n",
    "     round(avg('AQ4'),2).alias('meanA4'), round(std('AQ4'),2).alias('stdA4'), \\\n",
    "     round(avg('AFinal'),2).alias('meanAF'), round(std('AFinal'),2).alias('stdAF'), \\\n",
    "     round(avg('HQ1'),2).alias('meanH1'), round(std('HQ1'),2).alias('stdH1'), \\\n",
    "     round(avg('HQ2'),2).alias('meanH2'), round(std('HQ2'),2).alias('stdH2'), \\\n",
    "     round(avg('HQ3'),2).alias('meanH3'), round(std('HQ3'),2).alias('stdH3'), \\\n",
    "     round(avg('HQ4'),2).alias('meanH4'), round(std('HQ4'),2).alias('stdH4'), \\\n",
    "     round(avg('HFinal'),2).alias('meanHF'), round(std('HFinal'),2).alias('stdHF')) \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc023dd8-7648-438c-875c-a541a9c3c28b",
   "metadata": {},
   "source": [
    "And if we want to group these calculations, we can simply add a ```.groupby()``` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e59243f7-df68-43c5-8687-503a1c658ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+\n",
      "|season|meanA1|stdA1|meanA2|stdA2|meanA3|stdA3|meanA4|stdA4|meanAF|stdAF|meanH1|stdH1|meanH2|stdH2|meanH3|stdH3|meanH4|stdH4|meanHF|stdHF|\n",
      "+------+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+\n",
      "|  2002|  4.04| 4.47|  6.02| 5.17|  4.45| 4.64|  5.93| 5.28| 20.64| 10.3|  4.31| 4.55|  7.41| 5.92|  4.72| 4.72|  6.43| 5.38| 23.02| 10.3|\n",
      "|  2003|  3.56| 4.32|  6.09| 5.44|  3.91|  4.4|  5.36| 5.15| 19.11|10.21|  5.03| 4.68|  6.63| 5.37|  4.56| 4.72|   6.3| 5.32| 22.68|10.11|\n",
      "|  2004|  3.91| 4.56|  6.27| 5.09|  4.15| 4.63|   5.8|  5.4| 20.24|10.16|  4.94| 4.73|  7.07| 5.59|  4.25| 4.68|  6.56| 5.16| 22.91|10.44|\n",
      "|  2005|  3.89| 4.52|  5.54| 4.95|  4.03| 4.48|  5.19| 4.75| 18.79| 9.93|  4.39| 4.59|  7.43|  5.7|  4.68| 4.49|  5.74| 5.17| 22.31| 9.77|\n",
      "|  2006|  3.63| 3.99|  6.18| 5.16|   4.3| 4.53|  6.06| 5.54| 20.25|10.27|  4.61|  5.0|  6.04|  5.2|  4.57| 4.93|  5.93| 5.26| 21.26| 9.88|\n",
      "|  2007|   3.7| 4.32|  6.22| 5.05|  4.34| 4.51|  5.87| 5.28| 20.23|10.57|  5.03| 4.82|  7.07| 5.78|  4.91| 4.46|  6.02| 6.01| 23.16| 10.5|\n",
      "|  2008|   3.8|  4.2|   6.5| 5.34|  4.07| 4.24|  6.42|  5.5| 20.84|10.28|  5.18| 4.86|  7.21| 5.83|   4.6|  4.7|  6.04| 5.17| 23.18|10.41|\n",
      "|  2009|  3.87| 4.75|   6.2| 5.29|   4.3| 4.71|  5.94| 5.57| 20.38|10.74|  4.74| 4.73|  7.79| 6.21|  4.23| 4.72|  5.89| 5.18| 22.78|10.79|\n",
      "|  2010|  3.97| 4.73|  6.87|  5.4|  4.63| 4.78|   5.7| 4.95| 21.32|10.28|  4.58| 4.49|  6.77| 5.48|  4.87| 4.75|  6.68| 5.63|  23.0|10.23|\n",
      "|  2011|  3.86| 4.46|  5.98| 5.28|  4.58| 4.63|   6.0| 5.12| 20.51| 9.65|  5.02| 4.86|  7.33| 5.67|  5.03| 4.96|  6.48| 5.26| 23.98|10.53|\n",
      "|  2012|  4.47| 4.75|  6.27| 5.45|  4.51| 4.63|  6.24| 5.34| 21.66|10.37|  4.93|  4.5|   7.0| 5.38|  5.22| 4.54|  6.82| 5.68| 24.12|10.56|\n",
      "|  2013|  4.04| 4.35|  6.67| 5.44|  5.01|  4.9|  6.18| 5.42| 21.98| 9.68|  5.11| 4.79|  7.45| 5.73|  5.26| 4.85|  6.93| 5.76| 24.89|10.58|\n",
      "|  2014|  4.28| 4.89|  6.32| 4.77|  4.73| 5.06|  5.88| 5.24|  21.3| 9.78|  4.91| 4.82|  7.16| 6.11|   5.4| 5.18|  6.37| 5.33| 23.97|10.87|\n",
      "+------+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nfl_sp.select(['AQ1', 'AQ2', 'AQ3', 'AQ4', 'AFinal', 'HQ1', 'HQ2', 'HQ3', 'HQ4', 'HFinal', 'season']) \\\n",
    ".groupBy('season') \\\n",
    ".agg(round(avg('AQ1'),2).alias('meanA1'), round(std('AQ1'),2).alias('stdA1'), \\\n",
    "     round(avg('AQ2'),2).alias('meanA2'), round(std('AQ2'),2).alias('stdA2'), \\\n",
    "     round(avg('AQ3'),2).alias('meanA3'), round(std('AQ3'),2).alias('stdA3'), \\\n",
    "     round(avg('AQ4'),2).alias('meanA4'), round(std('AQ4'),2).alias('stdA4'), \\\n",
    "     round(avg('AFinal'),2).alias('meanAF'), round(std('AFinal'),2).alias('stdAF'), \\\n",
    "     round(avg('HQ1'),2).alias('meanH1'), round(std('HQ1'),2).alias('stdH1'), \\\n",
    "     round(avg('HQ2'),2).alias('meanH2'), round(std('HQ2'),2).alias('stdH2'), \\\n",
    "     round(avg('HQ3'),2).alias('meanH3'), round(std('HQ3'),2).alias('stdH3'), \\\n",
    "     round(avg('HQ4'),2).alias('meanH4'), round(std('HQ4'),2).alias('stdH4'), \\\n",
    "     round(avg('HFinal'),2).alias('meanHF'), round(std('HFinal'),2).alias('stdHF')) \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67450c62-1e63-4662-a610-42f06db0c37d",
   "metadata": {},
   "source": [
    "And that's it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5091794f-d74c-4d19-bebe-9d6c8c536e4d",
   "metadata": {},
   "source": [
    "# Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a528e26-be0a-4a1b-93dd-fd572675ca9e",
   "metadata": {},
   "source": [
    "Now let's perform the same calculations using 'pandas-on-spark' from ```pyspark```.\n",
    "\n",
    "We'll start by importing the ```pandas``` functionality from ```pyspark``` and then importing our data set as a pandas-spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8f877f68-2b45-4d74-b114-2834e54b3bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c9c94683-64c2-4a66-8cd0-4fb261ba3ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_ps = ps.from_pandas(nfl_data)\n",
    "# nfl_ps.head() # we can utilize a lot of the same pandas functions, including .head() to check that the data read in correctly (it did!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ebce1d-00b9-4253-bf68-e4b9b58b94f9",
   "metadata": {},
   "source": [
    "Since we can utilize a lot of the same pandas functions, we can use ```.describe()``` over our selected columns to return the **mean** and **standard deviation**.  Since ```.describe()``` returns a number of data summaries, I subsetted the result to only return **mean** and **std**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1aef1ba6-e04a-437f-9c92-0950c2cf1d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AQ1</th>\n",
       "      <th>AQ2</th>\n",
       "      <th>AQ3</th>\n",
       "      <th>AQ4</th>\n",
       "      <th>AFinal</th>\n",
       "      <th>HQ1</th>\n",
       "      <th>HQ2</th>\n",
       "      <th>HQ3</th>\n",
       "      <th>HQ4</th>\n",
       "      <th>HFinal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.924806</td>\n",
       "      <td>6.241429</td>\n",
       "      <td>4.386920</td>\n",
       "      <td>5.890233</td>\n",
       "      <td>20.557188</td>\n",
       "      <td>4.828868</td>\n",
       "      <td>7.105157</td>\n",
       "      <td>4.791126</td>\n",
       "      <td>6.322962</td>\n",
       "      <td>23.174013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.490700</td>\n",
       "      <td>5.221593</td>\n",
       "      <td>4.632717</td>\n",
       "      <td>5.278775</td>\n",
       "      <td>10.195586</td>\n",
       "      <td>4.726903</td>\n",
       "      <td>5.702788</td>\n",
       "      <td>4.755145</td>\n",
       "      <td>5.417310</td>\n",
       "      <td>10.405952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           AQ1       AQ2       AQ3       AQ4     AFinal       HQ1       HQ2       HQ3       HQ4     HFinal\n",
       "mean  3.924806  6.241429  4.386920  5.890233  20.557188  4.828868  7.105157  4.791126  6.322962  23.174013\n",
       "std   4.490700  5.221593  4.632717  5.278775  10.195586  4.726903  5.702788  4.755145  5.417310  10.405952"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfl_ps[['AQ1', 'AQ2', 'AQ3', 'AQ4', 'AFinal', 'HQ1', 'HQ2', 'HQ3', 'HQ4', 'HFinal']].describe()[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3708548-2712-42af-adda-db26a51b9720",
   "metadata": {},
   "source": [
    "To get the same summaries subsetted by year, we'll create two separate dataframes - one giving us the **mean** of our variables grouped by seasons and the other giving us the **std** of our variables grouped by season. Then we simply join the two data frames with ```.join()```. We can distinguish the column names using 'rsuffix' and 'lsuffix.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e8d3088a-0d97-4996-819e-4909cf2aab59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/pandas/groupby.py:649: FutureWarning: Default value of `numeric_only` will be changed to `False` instead of `True` in 4.0.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AQ1avg</th>\n",
       "      <th>AQ2avg</th>\n",
       "      <th>AQ3avg</th>\n",
       "      <th>AQ4avg</th>\n",
       "      <th>AFinalavg</th>\n",
       "      <th>HQ1avg</th>\n",
       "      <th>HQ2avg</th>\n",
       "      <th>HQ3avg</th>\n",
       "      <th>HQ4avg</th>\n",
       "      <th>HFinalavg</th>\n",
       "      <th>AQ1std</th>\n",
       "      <th>AQ2std</th>\n",
       "      <th>AQ3std</th>\n",
       "      <th>AQ4std</th>\n",
       "      <th>AFinalstd</th>\n",
       "      <th>HQ1std</th>\n",
       "      <th>HQ2std</th>\n",
       "      <th>HQ3std</th>\n",
       "      <th>HQ4std</th>\n",
       "      <th>HFinalstd</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>season</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>4.037453</td>\n",
       "      <td>6.022472</td>\n",
       "      <td>4.449438</td>\n",
       "      <td>5.928839</td>\n",
       "      <td>20.640449</td>\n",
       "      <td>4.307116</td>\n",
       "      <td>7.411985</td>\n",
       "      <td>4.715356</td>\n",
       "      <td>6.426966</td>\n",
       "      <td>23.018727</td>\n",
       "      <td>4.470297</td>\n",
       "      <td>5.167810</td>\n",
       "      <td>4.639673</td>\n",
       "      <td>5.282133</td>\n",
       "      <td>10.296996</td>\n",
       "      <td>4.549980</td>\n",
       "      <td>5.923282</td>\n",
       "      <td>4.716938</td>\n",
       "      <td>5.384231</td>\n",
       "      <td>10.295065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>3.558052</td>\n",
       "      <td>6.093633</td>\n",
       "      <td>3.913858</td>\n",
       "      <td>5.355805</td>\n",
       "      <td>19.112360</td>\n",
       "      <td>5.026217</td>\n",
       "      <td>6.625468</td>\n",
       "      <td>4.561798</td>\n",
       "      <td>6.295880</td>\n",
       "      <td>22.677903</td>\n",
       "      <td>4.320719</td>\n",
       "      <td>5.440954</td>\n",
       "      <td>4.396264</td>\n",
       "      <td>5.148618</td>\n",
       "      <td>10.211049</td>\n",
       "      <td>4.681919</td>\n",
       "      <td>5.367875</td>\n",
       "      <td>4.719871</td>\n",
       "      <td>5.321832</td>\n",
       "      <td>10.105887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>3.913858</td>\n",
       "      <td>6.265918</td>\n",
       "      <td>4.153558</td>\n",
       "      <td>5.797753</td>\n",
       "      <td>20.239700</td>\n",
       "      <td>4.943820</td>\n",
       "      <td>7.071161</td>\n",
       "      <td>4.250936</td>\n",
       "      <td>6.561798</td>\n",
       "      <td>22.906367</td>\n",
       "      <td>4.556669</td>\n",
       "      <td>5.094638</td>\n",
       "      <td>4.631822</td>\n",
       "      <td>5.400181</td>\n",
       "      <td>10.162710</td>\n",
       "      <td>4.733562</td>\n",
       "      <td>5.587193</td>\n",
       "      <td>4.675238</td>\n",
       "      <td>5.155379</td>\n",
       "      <td>10.441145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>3.887640</td>\n",
       "      <td>5.543071</td>\n",
       "      <td>4.029963</td>\n",
       "      <td>5.191011</td>\n",
       "      <td>18.786517</td>\n",
       "      <td>4.393258</td>\n",
       "      <td>7.426966</td>\n",
       "      <td>4.677903</td>\n",
       "      <td>5.737828</td>\n",
       "      <td>22.314607</td>\n",
       "      <td>4.522554</td>\n",
       "      <td>4.947945</td>\n",
       "      <td>4.476236</td>\n",
       "      <td>4.753811</td>\n",
       "      <td>9.926578</td>\n",
       "      <td>4.586556</td>\n",
       "      <td>5.704940</td>\n",
       "      <td>4.489042</td>\n",
       "      <td>5.173548</td>\n",
       "      <td>9.772905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>3.629213</td>\n",
       "      <td>6.179775</td>\n",
       "      <td>4.303371</td>\n",
       "      <td>6.063670</td>\n",
       "      <td>20.254682</td>\n",
       "      <td>4.606742</td>\n",
       "      <td>6.041199</td>\n",
       "      <td>4.565543</td>\n",
       "      <td>5.932584</td>\n",
       "      <td>21.258427</td>\n",
       "      <td>3.986015</td>\n",
       "      <td>5.157435</td>\n",
       "      <td>4.531605</td>\n",
       "      <td>5.539984</td>\n",
       "      <td>10.269065</td>\n",
       "      <td>4.999138</td>\n",
       "      <td>5.199966</td>\n",
       "      <td>4.931715</td>\n",
       "      <td>5.262574</td>\n",
       "      <td>9.876525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>3.696629</td>\n",
       "      <td>6.220974</td>\n",
       "      <td>4.337079</td>\n",
       "      <td>5.872659</td>\n",
       "      <td>20.228464</td>\n",
       "      <td>5.026217</td>\n",
       "      <td>7.074906</td>\n",
       "      <td>4.913858</td>\n",
       "      <td>6.018727</td>\n",
       "      <td>23.157303</td>\n",
       "      <td>4.324448</td>\n",
       "      <td>5.050111</td>\n",
       "      <td>4.507997</td>\n",
       "      <td>5.283565</td>\n",
       "      <td>10.572177</td>\n",
       "      <td>4.824287</td>\n",
       "      <td>5.780607</td>\n",
       "      <td>4.455721</td>\n",
       "      <td>6.010300</td>\n",
       "      <td>10.500518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>3.801498</td>\n",
       "      <td>6.498127</td>\n",
       "      <td>4.074906</td>\n",
       "      <td>6.423221</td>\n",
       "      <td>20.842697</td>\n",
       "      <td>5.179775</td>\n",
       "      <td>7.205993</td>\n",
       "      <td>4.595506</td>\n",
       "      <td>6.044944</td>\n",
       "      <td>23.183521</td>\n",
       "      <td>4.203686</td>\n",
       "      <td>5.337747</td>\n",
       "      <td>4.242863</td>\n",
       "      <td>5.498864</td>\n",
       "      <td>10.279806</td>\n",
       "      <td>4.857120</td>\n",
       "      <td>5.825685</td>\n",
       "      <td>4.700950</td>\n",
       "      <td>5.171299</td>\n",
       "      <td>10.414349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>3.868914</td>\n",
       "      <td>6.202247</td>\n",
       "      <td>4.299625</td>\n",
       "      <td>5.943820</td>\n",
       "      <td>20.382022</td>\n",
       "      <td>4.737828</td>\n",
       "      <td>7.790262</td>\n",
       "      <td>4.228464</td>\n",
       "      <td>5.887640</td>\n",
       "      <td>22.779026</td>\n",
       "      <td>4.748728</td>\n",
       "      <td>5.285488</td>\n",
       "      <td>4.714412</td>\n",
       "      <td>5.571192</td>\n",
       "      <td>10.743600</td>\n",
       "      <td>4.731771</td>\n",
       "      <td>6.208245</td>\n",
       "      <td>4.723587</td>\n",
       "      <td>5.176810</td>\n",
       "      <td>10.788110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>3.973783</td>\n",
       "      <td>6.865169</td>\n",
       "      <td>4.629213</td>\n",
       "      <td>5.704120</td>\n",
       "      <td>21.318352</td>\n",
       "      <td>4.576779</td>\n",
       "      <td>6.771536</td>\n",
       "      <td>4.868914</td>\n",
       "      <td>6.681648</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>4.727466</td>\n",
       "      <td>5.396722</td>\n",
       "      <td>4.784959</td>\n",
       "      <td>4.953022</td>\n",
       "      <td>10.278809</td>\n",
       "      <td>4.485221</td>\n",
       "      <td>5.476904</td>\n",
       "      <td>4.753476</td>\n",
       "      <td>5.626849</td>\n",
       "      <td>10.230060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>3.857678</td>\n",
       "      <td>5.981273</td>\n",
       "      <td>4.584270</td>\n",
       "      <td>5.996255</td>\n",
       "      <td>20.509363</td>\n",
       "      <td>5.022472</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>5.033708</td>\n",
       "      <td>6.479401</td>\n",
       "      <td>23.981273</td>\n",
       "      <td>4.455541</td>\n",
       "      <td>5.281869</td>\n",
       "      <td>4.628635</td>\n",
       "      <td>5.120722</td>\n",
       "      <td>9.647690</td>\n",
       "      <td>4.859633</td>\n",
       "      <td>5.673886</td>\n",
       "      <td>4.962527</td>\n",
       "      <td>5.262521</td>\n",
       "      <td>10.527962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>4.468165</td>\n",
       "      <td>6.273408</td>\n",
       "      <td>4.509363</td>\n",
       "      <td>6.235955</td>\n",
       "      <td>21.655431</td>\n",
       "      <td>4.928839</td>\n",
       "      <td>7.003745</td>\n",
       "      <td>5.217228</td>\n",
       "      <td>6.823970</td>\n",
       "      <td>24.119850</td>\n",
       "      <td>4.754293</td>\n",
       "      <td>5.452819</td>\n",
       "      <td>4.627771</td>\n",
       "      <td>5.338234</td>\n",
       "      <td>10.367006</td>\n",
       "      <td>4.504654</td>\n",
       "      <td>5.379925</td>\n",
       "      <td>4.542779</td>\n",
       "      <td>5.675672</td>\n",
       "      <td>10.559919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>4.044944</td>\n",
       "      <td>6.670412</td>\n",
       "      <td>5.011236</td>\n",
       "      <td>6.183521</td>\n",
       "      <td>21.977528</td>\n",
       "      <td>5.112360</td>\n",
       "      <td>7.445693</td>\n",
       "      <td>5.258427</td>\n",
       "      <td>6.932584</td>\n",
       "      <td>24.891386</td>\n",
       "      <td>4.345709</td>\n",
       "      <td>5.441761</td>\n",
       "      <td>4.904719</td>\n",
       "      <td>5.417879</td>\n",
       "      <td>9.680199</td>\n",
       "      <td>4.791373</td>\n",
       "      <td>5.726854</td>\n",
       "      <td>4.846194</td>\n",
       "      <td>5.757240</td>\n",
       "      <td>10.582623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>4.284644</td>\n",
       "      <td>6.322097</td>\n",
       "      <td>4.734082</td>\n",
       "      <td>5.876404</td>\n",
       "      <td>21.295880</td>\n",
       "      <td>4.913858</td>\n",
       "      <td>7.164794</td>\n",
       "      <td>5.397004</td>\n",
       "      <td>6.374532</td>\n",
       "      <td>23.973783</td>\n",
       "      <td>4.893746</td>\n",
       "      <td>4.769223</td>\n",
       "      <td>5.059837</td>\n",
       "      <td>5.243299</td>\n",
       "      <td>9.780606</td>\n",
       "      <td>4.821248</td>\n",
       "      <td>6.113817</td>\n",
       "      <td>5.184534</td>\n",
       "      <td>5.329217</td>\n",
       "      <td>10.870879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          AQ1avg    AQ2avg    AQ3avg    AQ4avg  AFinalavg    HQ1avg    HQ2avg    HQ3avg    HQ4avg  HFinalavg    AQ1std    AQ2std    AQ3std    AQ4std  AFinalstd    HQ1std    HQ2std    HQ3std    HQ4std  HFinalstd\n",
       "season                                                                                                                                                                                                            \n",
       "2002    4.037453  6.022472  4.449438  5.928839  20.640449  4.307116  7.411985  4.715356  6.426966  23.018727  4.470297  5.167810  4.639673  5.282133  10.296996  4.549980  5.923282  4.716938  5.384231  10.295065\n",
       "2003    3.558052  6.093633  3.913858  5.355805  19.112360  5.026217  6.625468  4.561798  6.295880  22.677903  4.320719  5.440954  4.396264  5.148618  10.211049  4.681919  5.367875  4.719871  5.321832  10.105887\n",
       "2004    3.913858  6.265918  4.153558  5.797753  20.239700  4.943820  7.071161  4.250936  6.561798  22.906367  4.556669  5.094638  4.631822  5.400181  10.162710  4.733562  5.587193  4.675238  5.155379  10.441145\n",
       "2005    3.887640  5.543071  4.029963  5.191011  18.786517  4.393258  7.426966  4.677903  5.737828  22.314607  4.522554  4.947945  4.476236  4.753811   9.926578  4.586556  5.704940  4.489042  5.173548   9.772905\n",
       "2006    3.629213  6.179775  4.303371  6.063670  20.254682  4.606742  6.041199  4.565543  5.932584  21.258427  3.986015  5.157435  4.531605  5.539984  10.269065  4.999138  5.199966  4.931715  5.262574   9.876525\n",
       "2007    3.696629  6.220974  4.337079  5.872659  20.228464  5.026217  7.074906  4.913858  6.018727  23.157303  4.324448  5.050111  4.507997  5.283565  10.572177  4.824287  5.780607  4.455721  6.010300  10.500518\n",
       "2008    3.801498  6.498127  4.074906  6.423221  20.842697  5.179775  7.205993  4.595506  6.044944  23.183521  4.203686  5.337747  4.242863  5.498864  10.279806  4.857120  5.825685  4.700950  5.171299  10.414349\n",
       "2009    3.868914  6.202247  4.299625  5.943820  20.382022  4.737828  7.790262  4.228464  5.887640  22.779026  4.748728  5.285488  4.714412  5.571192  10.743600  4.731771  6.208245  4.723587  5.176810  10.788110\n",
       "2010    3.973783  6.865169  4.629213  5.704120  21.318352  4.576779  6.771536  4.868914  6.681648  23.000000  4.727466  5.396722  4.784959  4.953022  10.278809  4.485221  5.476904  4.753476  5.626849  10.230060\n",
       "2011    3.857678  5.981273  4.584270  5.996255  20.509363  5.022472  7.333333  5.033708  6.479401  23.981273  4.455541  5.281869  4.628635  5.120722   9.647690  4.859633  5.673886  4.962527  5.262521  10.527962\n",
       "2012    4.468165  6.273408  4.509363  6.235955  21.655431  4.928839  7.003745  5.217228  6.823970  24.119850  4.754293  5.452819  4.627771  5.338234  10.367006  4.504654  5.379925  4.542779  5.675672  10.559919\n",
       "2013    4.044944  6.670412  5.011236  6.183521  21.977528  5.112360  7.445693  5.258427  6.932584  24.891386  4.345709  5.441761  4.904719  5.417879   9.680199  4.791373  5.726854  4.846194  5.757240  10.582623\n",
       "2014    4.284644  6.322097  4.734082  5.876404  21.295880  4.913858  7.164794  5.397004  6.374532  23.973783  4.893746  4.769223  5.059837  5.243299   9.780606  4.821248  6.113817  5.184534  5.329217  10.870879"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfl_ps[['AQ1', 'AQ2', 'AQ3', 'AQ4', 'AFinal', 'HQ1', 'HQ2', 'HQ3', 'HQ4', 'HFinal', 'season']].groupby('season').mean() \\\n",
    ".join(nfl_ps[['AQ1', 'AQ2', 'AQ3', 'AQ4', 'AFinal', 'HQ1', 'HQ2', 'HQ3', 'HQ4', 'HFinal', 'season']].groupby('season').std(), rsuffix='std', lsuffix='avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a95d33-b70d-4fff-9abc-babe683cfecd",
   "metadata": {},
   "source": [
    "And that's it!\n",
    "\n",
    "It looks like a lot of built-in features for 'spark SQL' and 'pandas-on-spark' work seamlessly with ```pySparks``` RDD - Resilient Distributed Datasets. Since data in spark is stored in RDDs, the data is spread out in 'chunks' and in order to perform any action on the data, the action has to be performed on each 'chunk' and then the 'chunks' are combined - much like what we did in **Part 2**. But with 'spark SQL' and 'pandas-on-spark', we don't have to utilize ```map/reduce``` on all of our actions - these API's are built so that much of ```map/reduce``` is happening in the background and we can interact with spark data using the same functionality we know from ```pandas``` and *SQL*."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
