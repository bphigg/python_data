{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3005e3ff-ae64-4abc-9ac0-30473dde833e",
   "metadata": {},
   "source": [
    "# Modeling with Data Streams\n",
    "brian higginbotham"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197de0e8-81e3-4970-b64f-756c82857ca7",
   "metadata": {},
   "source": [
    "For this project I will explore reading, summarizing, and modeling with data streams with the goal of writing the results of these operations to the console.\n",
    "\n",
    "The data is from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/849/power+consumption+of+tetouan+city). It is power usage data from the Tunisian city of Tetouan, measuring climate conditions as well as the power output in three zones. Measurements were taken every 10 minutes from January 1 to December 30, 2017.\n",
    "\n",
    "In the Modeling section, we will use the third power zone as our target and all other columns will be used as our predictiors or features.\n",
    "\n",
    "We'll first import some common libraries to get us started on reading and summarizing the data. We'll add additional libraries as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b775ec6e-8fa3-456b-ad0c-f3d8166792fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e13b0c7-de2d-4462-a863-8a5240389555",
   "metadata": {},
   "source": [
    "# Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffdf79d-3ee5-4d73-b0ff-2388946f37f4",
   "metadata": {},
   "source": [
    "Import the data as a ```pandas``` dataframe and then save as a *pandas-on-spark* dataframe. We'll use the pandas functionality to get a few summaries such as mean, median, standard deviation and even some correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60190940-92f6-47df-8c99-fc16b0bc77b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_pd = pd.read_csv('power_ml_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5921e9d7-cbde-4f08-b717-1ac567e35f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind_Speed</th>\n",
       "      <th>General_Diffuse_Flows</th>\n",
       "      <th>Diffuse_Flows</th>\n",
       "      <th>Power_Zone_1</th>\n",
       "      <th>Power_Zone_2</th>\n",
       "      <th>Power_Zone_3</th>\n",
       "      <th>Month</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.559</td>\n",
       "      <td>73.8</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.119</td>\n",
       "      <td>34055.69620</td>\n",
       "      <td>16128.87538</td>\n",
       "      <td>20240.96386</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.414</td>\n",
       "      <td>74.5</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.085</td>\n",
       "      <td>29814.68354</td>\n",
       "      <td>19375.07599</td>\n",
       "      <td>20131.08434</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.313</td>\n",
       "      <td>74.5</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.100</td>\n",
       "      <td>29128.10127</td>\n",
       "      <td>19006.68693</td>\n",
       "      <td>19668.43373</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.121</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.096</td>\n",
       "      <td>28228.86076</td>\n",
       "      <td>18361.09422</td>\n",
       "      <td>18899.27711</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.921</td>\n",
       "      <td>75.7</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.085</td>\n",
       "      <td>27335.69620</td>\n",
       "      <td>17872.34043</td>\n",
       "      <td>18442.40964</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Temperature  Humidity  Wind_Speed  General_Diffuse_Flows  Diffuse_Flows  Power_Zone_1  Power_Zone_2  Power_Zone_3  Month  Hour\n",
       "0        6.559      73.8       0.083                  0.051          0.119   34055.69620   16128.87538   20240.96386      1     0\n",
       "1        6.414      74.5       0.083                  0.070          0.085   29814.68354   19375.07599   20131.08434      1     0\n",
       "2        6.313      74.5       0.080                  0.062          0.100   29128.10127   19006.68693   19668.43373      1     0\n",
       "3        6.121      75.0       0.083                  0.091          0.096   28228.86076   18361.09422   18899.27711      1     0\n",
       "4        5.921      75.7       0.081                  0.048          0.085   27335.69620   17872.34043   18442.40964      1     0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power_ps = ps.from_pandas(power_pd)\n",
    "power_ps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e885dc7-d482-4ec9-87d6-2c9ee2f56920",
   "metadata": {},
   "source": [
    "Use the ```.describe()``` method to get basic summaries for each column. We'll exlude the first row since it returns the count for each row, which is just the lenght of our dataset - about 47,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e61cecb9-e955-4ac5-996d-2cd819f2e993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind_Speed</th>\n",
       "      <th>General_Diffuse_Flows</th>\n",
       "      <th>Diffuse_Flows</th>\n",
       "      <th>Power_Zone_1</th>\n",
       "      <th>Power_Zone_2</th>\n",
       "      <th>Power_Zone_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>18.813220</td>\n",
       "      <td>68.288398</td>\n",
       "      <td>1.961621</td>\n",
       "      <td>182.531180</td>\n",
       "      <td>74.987211</td>\n",
       "      <td>32335.168690</td>\n",
       "      <td>21027.204976</td>\n",
       "      <td>17831.197608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.813341</td>\n",
       "      <td>15.560330</td>\n",
       "      <td>2.349351</td>\n",
       "      <td>264.431856</td>\n",
       "      <td>124.256146</td>\n",
       "      <td>7130.013305</td>\n",
       "      <td>5199.787153</td>\n",
       "      <td>6622.590470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.247000</td>\n",
       "      <td>11.340000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>13895.696200</td>\n",
       "      <td>8560.081466</td>\n",
       "      <td>5935.174070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>14.420000</td>\n",
       "      <td>58.320000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>0.122000</td>\n",
       "      <td>26290.632910</td>\n",
       "      <td>16957.317070</td>\n",
       "      <td>13121.927710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>18.780000</td>\n",
       "      <td>69.890000</td>\n",
       "      <td>0.086000</td>\n",
       "      <td>4.780000</td>\n",
       "      <td>4.284000</td>\n",
       "      <td>32261.596960</td>\n",
       "      <td>20804.863220</td>\n",
       "      <td>16405.282110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>22.910000</td>\n",
       "      <td>81.500000</td>\n",
       "      <td>4.915000</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>37317.446810</td>\n",
       "      <td>24698.734180</td>\n",
       "      <td>21628.915660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>40.010000</td>\n",
       "      <td>94.800000</td>\n",
       "      <td>6.483000</td>\n",
       "      <td>1163.000000</td>\n",
       "      <td>936.000000</td>\n",
       "      <td>52146.859050</td>\n",
       "      <td>37408.860760</td>\n",
       "      <td>47598.326360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Temperature   Humidity  Wind_Speed  General_Diffuse_Flows  Diffuse_Flows  Power_Zone_1  Power_Zone_2  Power_Zone_3\n",
       "mean    18.813220  68.288398    1.961621             182.531180      74.987211  32335.168690  21027.204976  17831.197608\n",
       "std      5.813341  15.560330    2.349351             264.431856     124.256146   7130.013305   5199.787153   6622.590470\n",
       "min      3.247000  11.340000    0.050000               0.004000       0.011000  13895.696200   8560.081466   5935.174070\n",
       "25%     14.420000  58.320000    0.078000               0.062000       0.122000  26290.632910  16957.317070  13121.927710\n",
       "50%     18.780000  69.890000    0.086000               4.780000       4.284000  32261.596960  20804.863220  16405.282110\n",
       "75%     22.910000  81.500000    4.915000             319.000000     101.000000  37317.446810  24698.734180  21628.915660\n",
       "max     40.010000  94.800000    6.483000            1163.000000     936.000000  52146.859050  37408.860760  47598.326360"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power_ps[['Temperature', 'Humidity', 'Wind_Speed', 'General_Diffuse_Flows', 'Diffuse_Flows', \\\n",
    "          'Power_Zone_1', 'Power_Zone_2', 'Power_Zone_3']].describe()[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c3b0d6-8977-43cd-b5d7-dc859796bfee",
   "metadata": {},
   "source": [
    "'Power_Zone_3', our target, has the lowest average of the three zones but is in the middle for standard deviations.\n",
    "\n",
    "Next, let's see if there is any correlation between the power zones and the other features. Utilize ```iloc()``` to only return the portion of the dataframe that we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59c018ad-a9c1-49fd-a204-8d8ed1b503ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Power_Zone_1</th>\n",
       "      <th>Power_Zone_2</th>\n",
       "      <th>Power_Zone_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Temperature</th>\n",
       "      <td>0.441446</td>\n",
       "      <td>0.384301</td>\n",
       "      <td>0.490752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Humidity</th>\n",
       "      <td>-0.289090</td>\n",
       "      <td>-0.297019</td>\n",
       "      <td>-0.234228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wind_Speed</th>\n",
       "      <td>0.166322</td>\n",
       "      <td>0.146338</td>\n",
       "      <td>0.279112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month</th>\n",
       "      <td>-0.006429</td>\n",
       "      <td>0.318368</td>\n",
       "      <td>-0.232978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hour</th>\n",
       "      <td>0.728118</td>\n",
       "      <td>0.663755</td>\n",
       "      <td>0.454806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>General_Diffuse_Flows</th>\n",
       "      <td>0.189994</td>\n",
       "      <td>0.158798</td>\n",
       "      <td>0.064942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diffuse_Flows</th>\n",
       "      <td>0.082885</td>\n",
       "      <td>0.047379</td>\n",
       "      <td>-0.036761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Power_Zone_1  Power_Zone_2  Power_Zone_3\n",
       "Temperature                0.441446      0.384301      0.490752\n",
       "Humidity                  -0.289090     -0.297019     -0.234228\n",
       "Wind_Speed                 0.166322      0.146338      0.279112\n",
       "Month                     -0.006429      0.318368     -0.232978\n",
       "Hour                       0.728118      0.663755      0.454806\n",
       "General_Diffuse_Flows      0.189994      0.158798      0.064942\n",
       "Diffuse_Flows              0.082885      0.047379     -0.036761"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power_ps[['Temperature', 'Humidity', 'Wind_Speed', 'Month', 'Hour', 'General_Diffuse_Flows', 'Diffuse_Flows', \\\n",
    "          'Power_Zone_1', 'Power_Zone_2', 'Power_Zone_3']].corr().iloc[:7,7:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f34f097-c874-4b64-ac08-d1488e033a9d",
   "metadata": {},
   "source": [
    "'Hour' has the strongest correlation with 'Power_Zone_1' while 'Power_Zone_3' has the strongest correlation with 'Temperature.'\n",
    "\n",
    "For the remainder of the summaries we'll convert the 'pandas-on-spark' dataframe to a 'spark-sql' dataframe. This way, we can utilize **SQL** language along with spark methods to produce more computationally intensive summaries.\n",
    "\n",
    "We'll also need to use 'spark-sql' for the model fitting and streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3544f526-5794-43f3-afaa-fb940a357dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+---------------------+-------------+------------+------------+------------+-----+----+\n",
      "|Temperature|Humidity|Wind_Speed|General_Diffuse_Flows|Diffuse_Flows|Power_Zone_1|Power_Zone_2|Power_Zone_3|Month|Hour|\n",
      "+-----------+--------+----------+---------------------+-------------+------------+------------+------------+-----+----+\n",
      "|      6.559|    73.8|     0.083|                0.051|        0.119|  34055.6962| 16128.87538| 20240.96386|    1|   0|\n",
      "|      6.414|    74.5|     0.083|                 0.07|        0.085| 29814.68354| 19375.07599| 20131.08434|    1|   0|\n",
      "|      6.313|    74.5|      0.08|                0.062|          0.1| 29128.10127| 19006.68693| 19668.43373|    1|   0|\n",
      "|      6.121|    75.0|     0.083|                0.091|        0.096| 28228.86076| 18361.09422| 18899.27711|    1|   0|\n",
      "|      5.921|    75.7|     0.081|                0.048|        0.085|  27335.6962| 17872.34043| 18442.40964|    1|   0|\n",
      "+-----------+--------+----------+---------------------+-------------+------------+------------+------------+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "power_spk = power_ps.to_spark()\n",
    "power_spk.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240f08a1-b6b3-499c-9266-1328f224ffb1",
   "metadata": {},
   "source": [
    "Here, we'll pull up a one-way contingency table, which is basically a count, of the Month variable. In order to get the output to be more readable, let's replace the month number with the month name. Also, for a one-way contingency table we'll need to create a dummy column with a constant value. We can do this at the end of our code using the ```.withColumn()``` method. To create the dummy variable, we'll just multiply each value in the 'Hour' column by zero and add one, creating a column of ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2442cd5-a2ce-47fe-aefe-10c584f95f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+\n",
      "|Month_cat_dummy|   1|\n",
      "+---------------+----+\n",
      "|           July|4029|\n",
      "|            Oct|4026|\n",
      "|            Sep|3913|\n",
      "|            Dec|3868|\n",
      "|          March|4057|\n",
      "|            Aug|3999|\n",
      "|            May|3997|\n",
      "|          April|3893|\n",
      "|           June|3913|\n",
      "|            Feb|3588|\n",
      "|            Nov|3877|\n",
      "|            Jan|4014|\n",
      "+---------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "power_spk.withColumn('Month_cat', when(power_spk.Month == 1, 'Jan').when(power_spk.Month == 2, 'Feb')\\\n",
    ".when(power_spk.Month == 3, 'March').when(power_spk.Month == 4, 'April').when(power_spk.Month == 5, 'May')\\\n",
    ".when(power_spk.Month == 6, 'June').when(power_spk.Month == 7, 'July').when(power_spk.Month == 8, 'Aug')\\\n",
    ".when(power_spk.Month == 9, 'Sep').when(power_spk.Month == 10, 'Oct').when(power_spk.Month == 11, 'Nov')\\\n",
    ".when(power_spk.Month == 12, 'Dec')).withColumn('dummy', col('Hour')*0 +1).crosstab(col1='Month_cat', col2='dummy')\\\n",
    ".show(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72452341-984f-49fc-bb97-5627d6c6f5d0",
   "metadata": {},
   "source": [
    "The 'Month' category looks pretty evenly distributed with about 4,000 entries for each month.\n",
    "\n",
    "Now let's do the same for the 'Hour' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16056306-38ff-46df-a5c7-4a43f183e64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|Hour_dummy|   1|\n",
      "+----------+----+\n",
      "|         7|1964|\n",
      "|        15|1947|\n",
      "|        11|1972|\n",
      "|         3|1966|\n",
      "|         8|1957|\n",
      "|        22|1966|\n",
      "|        16|1950|\n",
      "|         0|1950|\n",
      "|         5|1968|\n",
      "|        18|1955|\n",
      "|        17|1979|\n",
      "|         6|1992|\n",
      "+----------+----+\n",
      "only showing top 12 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "power_spk.withColumn('dummy', col('Hour')*0 +1).crosstab(col1='Hour', col2='dummy').show(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1204ed7d-d74c-4498-b2ce-bf3d03bc6596",
   "metadata": {},
   "source": [
    "We've limited the output here to 12, but we get a good sense that the 'Hour' category is also pretty evenly distributed with about 1950 entries for each hour.\n",
    "\n",
    "Now let's produce a two-way contingency table to look at the combination of 'Hour' and 'Month' columns. Again, we'll rename the 'Month' entries for easier readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e706425-e8e7-473c-a5bb-120efea4a077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|Month_cat_Hour|  0|  1| 10| 11| 12| 13| 14| 15| 16| 17| 18| 19|  2| 20| 21| 22| 23|  3|  4|  5|  6|  7|  8|  9|\n",
      "+--------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|          July|172|169|167|168|166|173|169|162|164|172|176|172|161|159|163|173|167|170|167|174|168|164|165|168|\n",
      "|           Oct|171|168|163|171|166|161|175|165|159|176|165|162|175|170|173|169|167|172|162|163|170|164|168|171|\n",
      "|           Sep|164|161|159|161|174|159|162|168|166|169|162|163|160|158|163|163|167|164|167|158|162|159|162|162|\n",
      "|           Dec|154|159|165|167|164|156|168|157|158|153|159|160|167|158|159|168|156|167|164|164|165|165|156|159|\n",
      "|         March|174|171|175|168|166|173|165|171|166|163|165|170|167|168|175|165|172|168|166|170|173|170|166|170|\n",
      "|           Aug|168|169|162|171|167|167|164|166|168|165|169|168|171|165|169|164|173|165|168|171|165|162|165|157|\n",
      "|           May|164|166|172|159|175|166|159|172|167|171|162|160|163|165|174|161|162|171|174|164|172|163|167|168|\n",
      "|         April|161|162|164|163|165|157|165|156|162|161|155|163|157|161|165|160|164|160|163|169|167|162|165|166|\n",
      "|          June|160|164|159|169|155|167|162|158|161|161|165|162|169|159|156|167|166|165|167|167|163|167|160|164|\n",
      "|           Feb|142|151|145|141|155|154|153|146|153|152|144|148|147|149|151|151|145|148|156|147|157|153|147|153|\n",
      "|           Nov|159|168|160|160|156|158|162|163|157|166|165|160|168|161|162|159|164|154|165|153|162|167|164|164|\n",
      "|           Jan|161|165|164|174|170|165|167|163|169|170|168|162|168|172|166|166|165|162|167|168|168|168|172|174|\n",
      "+--------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "power_spk.withColumn('Month_cat', when(power_spk.Month == 1, 'Jan').when(power_spk.Month == 2, 'Feb')\\\n",
    ".when(power_spk.Month == 3, 'March').when(power_spk.Month == 4, 'April').when(power_spk.Month == 5, 'May')\\\n",
    ".when(power_spk.Month == 6, 'June').when(power_spk.Month == 7, 'July').when(power_spk.Month == 8, 'Aug')\\\n",
    ".when(power_spk.Month == 9, 'Sep').when(power_spk.Month == 10, 'Oct').when(power_spk.Month == 11, 'Nov')\\\n",
    ".when(power_spk.Month == 12, 'Dec')).withColumn('dummy', col('Hour')*0 +1).crosstab(col1='Month_cat', col2='Hour')\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9c17c1-ebbd-497b-ba18-bef319bef2e8",
   "metadata": {},
   "source": [
    "It looks like our data is pretty evenly distributed between by 'Month' and 'Hour'.\n",
    "\n",
    "Now let's take a look at the monthly averages for all of our numeric variables. We can do this pretty easily with the ```.groupby()``` and ```.avg()``` methods. For easier readability, we'll run the resultant columns of the data frame through the ```round()``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bfbc044-4164-460d-8b37-d0fd68141dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+-------------+---------------+--------------------------+------------------+-----------------+-----------------+-----------------+----------+\n",
      "|Month|avg(Temperature)|avg(Humidity)|avg(Wind_Speed)|avg(General_Diffuse_Flows)|avg(Diffuse_Flows)|avg(Power_Zone_1)|avg(Power_Zone_2)|avg(Power_Zone_3)|avg(Month)|\n",
      "+-----+----------------+-------------+---------------+--------------------------+------------------+-----------------+-----------------+-----------------+----------+\n",
      "|    1|          12.735|       68.259|          0.702|                    103.96|            69.799|        31052.984|        19407.916|        17736.352|       1.0|\n",
      "|    2|          12.657|       66.491|          1.114|                   125.471|            92.331|        30973.863|        18774.586|        17309.708|       2.0|\n",
      "|    3|          14.584|       71.116|          1.006|                   181.402|            93.156|        31162.869|        18459.612|        16945.463|       3.0|\n",
      "|    4|          16.415|       75.408|          0.223|                   157.722|            83.495|        31143.207|        17600.307|        18574.918|       4.0|\n",
      "|    5|          20.301|       68.609|          2.307|                     274.5|           122.766|         32379.46|        19973.085|        17604.283|       5.0|\n",
      "|    6|          22.133|       68.761|          1.561|                   277.435|           103.228|        34573.227|        20649.035|         20416.13|       6.0|\n",
      "|    7|          27.201|       57.599|          4.642|                   294.112|            75.411|         35805.53|        24130.028|        28175.034|       7.0|\n",
      "|    8|           25.74|       66.023|          4.533|                   227.179|            67.106|        36436.262|        24657.025|        24684.369|       8.0|\n",
      "|    9|          22.641|       66.868|          2.947|                   202.202|            49.071|        33415.103|         20189.46|        14928.416|       9.0|\n",
      "|   10|          20.476|       71.524|          2.784|                   115.815|            46.629|        32806.993|         21457.89|        13266.437|      10.0|\n",
      "|   11|          16.819|       69.638|          1.259|                   121.915|            62.876|        28993.342|        23229.117|        12867.073|      11.0|\n",
      "|   12|          13.283|       69.237|          0.255|                   100.023|            34.305|        28959.103|        23618.565|         11017.12|      12.0|\n",
      "+-----+----------------+-------------+---------------+--------------------------+------------------+-----------------+-----------------+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnth_avg = power_spk.select(['Temperature', 'Humidity', 'Wind_Speed', 'General_Diffuse_Flows', 'Diffuse_Flows', \\\n",
    "          'Power_Zone_1', 'Power_Zone_2', 'Power_Zone_3', 'Month']).groupby('Month').avg()\n",
    "mnth_avg.select(*[round(c,3).alias(c) for c in mnth_avg.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d367fada-2338-4008-ac13-64f423776050",
   "metadata": {},
   "source": [
    "Next, we'll take a look at the standard deviation for each of the numerica variables. However, there is no convenient ```.std()``` method like we did with ```.avg()```. So we will utilize some basic **SQL** coding along with the ```.agg()``` method. Again, for readability, we'll use ```round()``` as well as ```.alias()``` to rename the resultant columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d000a3b3-40eb-4492-a647-6313fc5f5f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------------+--------+----------------+--------------+----------+----------+----------+\n",
      "|Month|temp_std|humidity_std|wind_std|gen_dif_flow_std|diff_flows_std|zone_1_std|zone_2_std|zone_3_std|\n",
      "+-----+--------+------------+--------+----------------+--------------+----------+----------+----------+\n",
      "|    1|   3.241|      12.156|   1.612|         166.165|       131.459|  7402.323|  4515.296|  4436.997|\n",
      "|    2|    2.62|      12.412|   1.981|          206.73|       169.156|  6874.585|  4390.391|  4353.976|\n",
      "|    3|   3.759|      13.918|   1.901|         260.149|       151.168|  6782.137|  4185.118|  4256.766|\n",
      "|    4|   2.806|      14.313|    0.82|         246.174|       123.912|    6496.7|  3835.629|  4556.263|\n",
      "|    5|     3.3|      16.436|   2.408|         331.999|       171.586|  6809.333|  4182.544|  4353.394|\n",
      "|    6|    2.69|      14.973|   2.235|         328.277|       143.498|  7317.808|  4465.664|  5596.703|\n",
      "|    7|   3.857|       18.85|   1.111|         331.734|        95.045|  6966.074|  4968.511|  6913.958|\n",
      "|    8|    2.95|      18.483|   1.301|         289.906|        90.663|  7054.722|  5163.443|  6520.956|\n",
      "|    9|   2.878|      15.993|   2.293|         270.173|        67.524|  6471.367|  4205.504|  3425.856|\n",
      "|   10|   2.987|      13.981|   2.399|         185.043|        69.421|  6479.133|  4612.671|  3087.806|\n",
      "|   11|   3.683|      12.832|   2.075|         184.692|       123.142|  5918.583|  5452.892|  3509.672|\n",
      "|   12|   3.349|      13.739|   0.894|         161.702|        56.394|  6176.465|  5713.467|  2840.343|\n",
      "+-----+--------+------------+--------+----------------+--------------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "power_spk.groupby('Month')\\\n",
    ".agg(round(std('Temperature'),3).alias('temp_std'), round(std('Humidity'),3).alias('humidity_std'),\n",
    "round(std('Wind_Speed'),3).alias('wind_std'), round(std('General_Diffuse_Flows'),3).alias('gen_dif_flow_std'),\n",
    "round(std('Diffuse_Flows'),3).alias('diff_flows_std'), round(std('Power_Zone_1'),3).alias('zone_1_std'),\n",
    "round(std('Power_Zone_2'),3).alias('zone_2_std'),round(std('Power_Zone_3'),3).alias('zone_3_std')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03ed033-270a-4539-bff1-46bda5b1fe63",
   "metadata": {},
   "source": [
    "'Power_Zone_1' has a much higher standard deviation than the other two power zones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bda7e81-2c84-4b9e-af9c-a5e7e9e2f632",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b95186c-2fe4-4d32-a24e-ffa7ea1b9c18",
   "metadata": {},
   "source": [
    "Now we'll need to prep the data for modeling. What we're going to do here is set up some transformations that will prepare the data set to be fitted to a model and then we will use that model to make some predictions. We can then compare those predictions to the actual results and measure how well our model performs.\n",
    "\n",
    "First off, let's look at our data formats to make sure it can be read correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31a1b9a8-e2d8-4647-a772-071884bf7109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Temperature: double (nullable = false)\n",
      " |-- Humidity: double (nullable = false)\n",
      " |-- Wind_Speed: double (nullable = false)\n",
      " |-- General_Diffuse_Flows: double (nullable = false)\n",
      " |-- Diffuse_Flows: double (nullable = false)\n",
      " |-- Power_Zone_1: double (nullable = false)\n",
      " |-- Power_Zone_2: double (nullable = false)\n",
      " |-- Power_Zone_3: double (nullable = false)\n",
      " |-- Month: long (nullable = false)\n",
      " |-- Hour: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "power_spk.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f3b4f1-f2fc-4b5b-96d1-16843a9a725b",
   "metadata": {},
   "source": [
    "In order to run the appropriate transformations on the 'Hour' column, we'll need to change it from *long* format to *double* format. We can use the ```.withColumn()``` method for this - it creates a new column by performing some action on an existing column. If we use the same column name for the new column as the old column, we essentially write over the old column with the new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffeb93a6-f8da-4dd9-8bbf-0767ddd3e9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Temperature: double (nullable = false)\n",
      " |-- Humidity: double (nullable = false)\n",
      " |-- Wind_Speed: double (nullable = false)\n",
      " |-- General_Diffuse_Flows: double (nullable = false)\n",
      " |-- Diffuse_Flows: double (nullable = false)\n",
      " |-- Power_Zone_1: double (nullable = false)\n",
      " |-- Power_Zone_2: double (nullable = false)\n",
      " |-- Power_Zone_3: double (nullable = false)\n",
      " |-- Month: long (nullable = false)\n",
      " |-- Hour: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "power_spk = power_spk.withColumn(\"Hour\",  \n",
    "                                  power_spk[\"Hour\"] \n",
    "                                  .cast('double')) \n",
    "power_spk.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c95ddc5-0e9f-433a-8fad-4d7476631c5e",
   "metadata": {},
   "source": [
    "Now we'll start our pipeline of transformations. Keep in mind that the goal for these transformations is get the data into a format so that pySpark can fit a model. To that end, we will need a **'label'** column that contains our target data ('Power_Zone_3') and a **'features'** column that will containi a vector of all the features to be used in fitting the model.\n",
    "\n",
    "Let's go ahead and import some modules to help us with the transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86a1af03-a5c8-4cfc-a23f-8299ebcd3fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import SQLTransformer, StringIndexer, Binarizer, VectorAssembler, \\\n",
    "VectorIndexer, OneHotEncoder, PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b222f571-5350-4782-a197-9090e77da87b",
   "metadata": {},
   "source": [
    "We'll start by creating a binary column distinguishing Day and Night. This will be based on the 'Hour' column. Using ```Binarizer()```, we can set a threshold at 6.5 which will assign the int **'0'** to all values below 6.5 and the int **'1'** to all values above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2055f4e-141b-4ac3-907a-0bb80fe19675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+---------------------+-------------+------------+------------+------------+-----+----+---------+\n",
      "|Temperature|Humidity|Wind_Speed|General_Diffuse_Flows|Diffuse_Flows|Power_Zone_1|Power_Zone_2|Power_Zone_3|Month|Hour|Night_Day|\n",
      "+-----------+--------+----------+---------------------+-------------+------------+------------+------------+-----+----+---------+\n",
      "|      6.559|    73.8|     0.083|                0.051|        0.119|  34055.6962| 16128.87538| 20240.96386|    1| 0.0|      0.0|\n",
      "|      6.414|    74.5|     0.083|                 0.07|        0.085| 29814.68354| 19375.07599| 20131.08434|    1| 0.0|      0.0|\n",
      "|      6.313|    74.5|      0.08|                0.062|          0.1| 29128.10127| 19006.68693| 19668.43373|    1| 0.0|      0.0|\n",
      "|      6.121|    75.0|     0.083|                0.091|        0.096| 28228.86076| 18361.09422| 18899.27711|    1| 0.0|      0.0|\n",
      "|      5.921|    75.7|     0.081|                0.048|        0.085|  27335.6962| 17872.34043| 18442.40964|    1| 0.0|      0.0|\n",
      "+-----------+--------+----------+---------------------+-------------+------------+------------+------------+-----+----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binary_HTrans = Binarizer(threshold = 6.5, inputCol='Hour', outputCol='Night_Day')\n",
    "binary_HTrans.transform(power_spk).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccb35ce-1397-4c79-b001-c4f2bb883cbd",
   "metadata": {},
   "source": [
    "We'd also like to use the 'Month' column as a categorical variable for our model. We could create dummy variables for each month, but since there are 12 our resultant data set would almost double in size. In order to keep the size and the resultant computational expense down, we use ```OneHotEncoder()``` on the 'Month' column. The result will be a vector that will be read/interpretted the same way a dummy variable would be, but contained within one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f436e5df-1119-4046-b6df-5902c49a1494",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+---------------------+-------------+------------+------------+------------+-----+----+---------+--------------+\n",
      "|Temperature|Humidity|Wind_Speed|General_Diffuse_Flows|Diffuse_Flows|Power_Zone_1|Power_Zone_2|Power_Zone_3|Month|Hour|Night_Day|     Month_OHE|\n",
      "+-----------+--------+----------+---------------------+-------------+------------+------------+------------+-----+----+---------+--------------+\n",
      "|      6.559|    73.8|     0.083|                0.051|        0.119|  34055.6962| 16128.87538| 20240.96386|    1| 0.0|      0.0|(12,[1],[1.0])|\n",
      "|      6.414|    74.5|     0.083|                 0.07|        0.085| 29814.68354| 19375.07599| 20131.08434|    1| 0.0|      0.0|(12,[1],[1.0])|\n",
      "|      6.313|    74.5|      0.08|                0.062|          0.1| 29128.10127| 19006.68693| 19668.43373|    1| 0.0|      0.0|(12,[1],[1.0])|\n",
      "|      6.121|    75.0|     0.083|                0.091|        0.096| 28228.86076| 18361.09422| 18899.27711|    1| 0.0|      0.0|(12,[1],[1.0])|\n",
      "|      5.921|    75.7|     0.081|                0.048|        0.085|  27335.6962| 17872.34043| 18442.40964|    1| 0.0|      0.0|(12,[1],[1.0])|\n",
      "+-----------+--------+----------+---------------------+-------------+------------+------------+------------+-----+----+---------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder(inputCol=\"Month\", outputCol=\"Month_OHE\")\n",
    "encoder_Trans = encoder.fit(power_spk)\n",
    "encoder_Trans.transform(binary_HTrans.transform(power_spk)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feea7be-82e6-4b39-8d69-fb027eee2d8d",
   "metadata": {},
   "source": [
    "Next, we'll perform a **Principal Component Analysis** on the reamaining columns, excepting the 'Power_Zone' columns. **PCA** takes the input columns and analyzes each row to produce a reduced version of the data - that's what is referred to as the 'principal components.' **PCA** is helpful in understanding the relationship between features when a data set has a large number of features. It also helps computationally by reducing the number of features while retaining most of the information about the relationship between those features.\n",
    "\n",
    "To perform **PCA** on our selected columns, we'll need to assmble the data in those columns into a vector. For this, we'll use ```VectorAssembler()``` and then create a new data object with the transformations made thus far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5172877b-80e9-4188-b12f-75952dada7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+---------------------+-------------+------------+------------+------------+-----+----+---------+--------------+--------------------+\n",
      "|Temperature|Humidity|Wind_Speed|General_Diffuse_Flows|Diffuse_Flows|Power_Zone_1|Power_Zone_2|Power_Zone_3|Month|Hour|Night_Day|     Month_OHE|            features|\n",
      "+-----------+--------+----------+---------------------+-------------+------------+------------+------------+-----+----+---------+--------------+--------------------+\n",
      "|      6.559|    73.8|     0.083|                0.051|        0.119|  34055.6962| 16128.87538| 20240.96386|    1| 0.0|      0.0|(12,[1],[1.0])|[6.559,73.8,0.083...|\n",
      "|      6.414|    74.5|     0.083|                 0.07|        0.085| 29814.68354| 19375.07599| 20131.08434|    1| 0.0|      0.0|(12,[1],[1.0])|[6.414,74.5,0.083...|\n",
      "|      6.313|    74.5|      0.08|                0.062|          0.1| 29128.10127| 19006.68693| 19668.43373|    1| 0.0|      0.0|(12,[1],[1.0])|[6.313,74.5,0.08,...|\n",
      "|      6.121|    75.0|     0.083|                0.091|        0.096| 28228.86076| 18361.09422| 18899.27711|    1| 0.0|      0.0|(12,[1],[1.0])|[6.121,75.0,0.083...|\n",
      "|      5.921|    75.7|     0.081|                0.048|        0.085|  27335.6962| 17872.34043| 18442.40964|    1| 0.0|      0.0|(12,[1],[1.0])|[5.921,75.7,0.081...|\n",
      "+-----------+--------+----------+---------------------+-------------+------------+------------+------------+-----+----+---------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler_ = VectorAssembler(inputCols=['Temperature', 'Humidity', 'Wind_Speed', 'General_Diffuse_Flows', 'Diffuse_Flows'], \n",
    "                             outputCol='features', handleInvalid='keep')\n",
    "power_featcol = assembler_.transform(encoder_Trans.transform(binary_HTrans.transform(power_spk)))\n",
    "power_featcol.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7dc56f-a8d0-4211-b9e8-a22adfcc3ca4",
   "metadata": {},
   "source": [
    "We'll set up a PCA object that we can fit the transformation on. Then we can fit the PCA Transformation to the new data object (since it has the appropriate 'features' column). But once the 'pca_Trans' is fit, we'll no longer need the new data object - we can then use 'pca_Trans' in our original pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6945ea30-f869-40a2-b171-492544d1f611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+---------------------+-------------+------------+------------+------------+-----+----+---------+--------------+--------------------+--------------------+\n",
      "|Temperature|Humidity|Wind_Speed|General_Diffuse_Flows|Diffuse_Flows|Power_Zone_1|Power_Zone_2|Power_Zone_3|Month|Hour|Night_Day|     Month_OHE|            features|                 pca|\n",
      "+-----------+--------+----------+---------------------+-------------+------------+------------+------------+-----+----+---------+--------------+--------------------+--------------------+\n",
      "|      6.559|    73.8|     0.083|                0.051|        0.119|  34055.6962| 16128.87538| 20240.96386|    1| 0.0|      0.0|(12,[1],[1.0])|[6.559,73.8,0.083...|[1.79440486365695...|\n",
      "|      6.414|    74.5|     0.083|                 0.07|        0.085| 29814.68354| 19375.07599| 20131.08434|    1| 0.0|      0.0|(12,[1],[1.0])|[6.414,74.5,0.083...|[1.80604083009823...|\n",
      "|      6.313|    74.5|      0.08|                0.062|          0.1| 29128.10127| 19006.68693| 19668.43373|    1| 0.0|      0.0|(12,[1],[1.0])|[6.313,74.5,0.08,...|[1.81022976305639...|\n",
      "|      6.121|    75.0|     0.083|                0.091|        0.096| 28228.86076| 18361.09422| 18899.27711|    1| 0.0|      0.0|(12,[1],[1.0])|[6.121,75.0,0.083...|[1.79866765174088...|\n",
      "|      5.921|    75.7|     0.081|                0.048|        0.085|  27335.6962| 17872.34043| 18442.40964|    1| 0.0|      0.0|(12,[1],[1.0])|[5.921,75.7,0.081...|[1.86328720163797...|\n",
      "+-----------+--------+----------+---------------------+-------------+------------+------------+------------+-----+----+---------+--------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(k=3, inputCol='features', outputCol=\"pca\")\n",
    "pca_Trans = pca.fit(power_featcol)\n",
    "pca_Trans.transform(assembler_.transform(encoder_Trans.transform(binary_HTrans.transform(power_spk)))).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9a46fe-dec0-4302-9a51-8620f18416d0",
   "metadata": {},
   "source": [
    "Now we can use ```SQLTransformer()``` to select only the columns we'll need for the model fit - 'Power_Zone_1', 'Power_Zone_2', the binary 'Night_Day' column, the one-hot-encoded 'Month' column', the 'pca' column, and the target column 'Power_Zone_3', which has been renamed as 'label.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e939acb3-b1a0-4952-9bfd-422f48128048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------+---------+--------------+--------------------+\n",
      "|Power_Zone_1|Power_Zone_2|      label|Night_Day|     Month_OHE|                 pca|\n",
      "+------------+------------+-----------+---------+--------------+--------------------+\n",
      "|  34055.6962| 16128.87538|20240.96386|      0.0|(12,[1],[1.0])|[1.79440486365695...|\n",
      "| 29814.68354| 19375.07599|20131.08434|      0.0|(12,[1],[1.0])|[1.80604083009823...|\n",
      "| 29128.10127| 19006.68693|19668.43373|      0.0|(12,[1],[1.0])|[1.81022976305639...|\n",
      "| 28228.86076| 18361.09422|18899.27711|      0.0|(12,[1],[1.0])|[1.79866765174088...|\n",
      "|  27335.6962| 17872.34043|18442.40964|      0.0|(12,[1],[1.0])|[1.86328720163797...|\n",
      "| 26624.81013| 17416.41337|18130.12048|      0.0|(12,[1],[1.0])|[1.87820674500461...|\n",
      "| 25998.98734| 16993.31307|17945.06024|      0.0|(12,[1],[1.0])|[1.91529298717955...|\n",
      "| 25446.07595| 16661.39818|17459.27711|      0.0|(12,[1],[1.0])|[1.92400540807029...|\n",
      "| 24777.72152| 16227.35562|17025.54217|      0.0|(12,[1],[1.0])|[1.89501930353027...|\n",
      "| 24279.49367| 15939.20973|16794.21687|      0.0|(12,[1],[1.0])|[1.88840748794691...|\n",
      "+------------+------------+-----------+---------+--------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlTrans_ = SQLTransformer(statement = '''SELECT Power_Zone_1, Power_Zone_2, Power_Zone_3 as label, Night_Day, \\\n",
    "Month_OHE, pca FROM __THIS__''')\n",
    "sqlTrans_.transform(pca_Trans.transform(assembler_.transform(encoder_Trans.transform\\\n",
    "                (binary_HTrans.transform(power_spk))))).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76417c4-449b-4e32-afd3-9c257fa4b7c0",
   "metadata": {},
   "source": [
    "Remember, the two columns pySpark needed to run a model fit are 'label' and 'features'. So we'll need to use 'VectorAssembler()' again to combine the feature columns into one column as a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "428ca11e-3820-475e-ad0e-d0086e8bb7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------+---------+--------------+--------------------+--------------------+\n",
      "|Power_Zone_1|Power_Zone_2|      label|Night_Day|     Month_OHE|                 pca|            features|\n",
      "+------------+------------+-----------+---------+--------------+--------------------+--------------------+\n",
      "|  34055.6962| 16128.87538|20240.96386|      0.0|(12,[1],[1.0])|[1.79440486365695...|(18,[0,1,4,15,16,...|\n",
      "| 29814.68354| 19375.07599|20131.08434|      0.0|(12,[1],[1.0])|[1.80604083009823...|(18,[0,1,4,15,16,...|\n",
      "| 29128.10127| 19006.68693|19668.43373|      0.0|(12,[1],[1.0])|[1.81022976305639...|(18,[0,1,4,15,16,...|\n",
      "| 28228.86076| 18361.09422|18899.27711|      0.0|(12,[1],[1.0])|[1.79866765174088...|(18,[0,1,4,15,16,...|\n",
      "|  27335.6962| 17872.34043|18442.40964|      0.0|(12,[1],[1.0])|[1.86328720163797...|(18,[0,1,4,15,16,...|\n",
      "| 26624.81013| 17416.41337|18130.12048|      0.0|(12,[1],[1.0])|[1.87820674500461...|(18,[0,1,4,15,16,...|\n",
      "| 25998.98734| 16993.31307|17945.06024|      0.0|(12,[1],[1.0])|[1.91529298717955...|(18,[0,1,4,15,16,...|\n",
      "| 25446.07595| 16661.39818|17459.27711|      0.0|(12,[1],[1.0])|[1.92400540807029...|(18,[0,1,4,15,16,...|\n",
      "| 24777.72152| 16227.35562|17025.54217|      0.0|(12,[1],[1.0])|[1.89501930353027...|(18,[0,1,4,15,16,...|\n",
      "| 24279.49367| 15939.20973|16794.21687|      0.0|(12,[1],[1.0])|[1.88840748794691...|(18,[0,1,4,15,16,...|\n",
      "+------------+------------+-----------+---------+--------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler_2 = VectorAssembler(inputCols=['Power_Zone_1', 'Power_Zone_2', 'Night_Day', 'Month_OHE', 'pca'], \n",
    "                             outputCol='features', handleInvalid='keep')\n",
    "assembler_2.transform(sqlTrans_.transform(pca_Trans.transform(assembler_.transform(encoder_Trans.\\\n",
    "                                transform(binary_HTrans.transform(power_spk)))))).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54394ab6-d5be-454d-a945-cdb4c30cf743",
   "metadata": {},
   "source": [
    "And now we're all set to fit the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb9e2f14-d1fd-42ae-a7c8-a6cc6a0122e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d11e738-c849-4dce-b575-5b586b5314e2",
   "metadata": {},
   "source": [
    "Set up the model instance - 'lr'. Set up a list of parameters in ```ParamGridBuilder()``` that ```CrossValidator()``` will iterate through to determine best fit. Since we are fitting an **ElasticNet** model we'll set up a list of 'regParams' that will determine the 'weight' of the penalty and a list of 'elasticNetParms' that will determine the ratio of the LASSO/Ridge models used in the ElasticNet model. We'll also create our pipeline instance that will run all the transormations that we set up above. We'll use the default **'rmse'** as the measurement of model performance in ```RegressionEvaluator()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1558a4d-f6a8-4871-a1d6-037529a5f8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "paramGrid_lr = ParamGridBuilder().addGrid(lr.regParam,[0, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.98, 0.99, 1])\\\n",
    ".addGrid(lr.elasticNetParam,[0, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.98, 0.99, 1])\\\n",
    ".build()\n",
    "pipeline_lr=Pipeline(stages=[binary_HTrans, encoder_Trans, assembler_, pca_Trans, sqlTrans_, assembler_2, lr])\n",
    "crossval_lr=CrossValidator(estimator=pipeline_lr,\n",
    "                        estimatorParamMaps=paramGrid_lr,\n",
    "                        evaluator=RegressionEvaluator(),\n",
    "                        numFolds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ec0411-f2da-407e-98bf-5a0f096d911d",
   "metadata": {},
   "source": [
    "Now we are ready to fit our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661bd35d-7aef-4b55-b286-0a97d85d91c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_lr_Model=crossval_lr.fit(power_spk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a583d3c-3388-46e7-aa31-77eec6e6a746",
   "metadata": {},
   "source": [
    "If your interested in viewing the cross validation results for the parameters, you can use the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "726e6900-6b97-4cd7-89c1-92ed721049b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_list = []\n",
    "#for i in range(len(paramGrid_lr)):\n",
    "#    my_list.append([cv_lr_Model.avgMetrics[i], paramGrid_lr[i].values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6b6d02-5d77-469f-9f66-2408abc1b30a",
   "metadata": {},
   "source": [
    "Now we can use our fitted model to transform the same data set to calculate predictions. We can see how well the model did by comparing its predictions to the actual value (in the 'label' column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71673002-a6c9-415e-b527-3fce55120eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2137.40482555687"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RegressionEvaluator(metricName='rmse').evaluate(cv_lr_Model.transform(power_spk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aea65db-6a56-4b9d-b301-976e3ddfc831",
   "metadata": {},
   "source": [
    "We may want to keep a record of the residual between the predictions and actual values. We can do that by creating a new column, 'residual', using the ```.withColumn()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae865aaa-cd61-4366-96a4-1f57fb2aa686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------+---------+--------------+--------------------+--------------------+------------------+------------------+\n",
      "|Power_Zone_1|Power_Zone_2|      label|Night_Day|     Month_OHE|                 pca|            features|        prediction|          residual|\n",
      "+------------+------------+-----------+---------+--------------+--------------------+--------------------+------------------+------------------+\n",
      "|  34055.6962| 16128.87538|20240.96386|      0.0|(12,[1],[1.0])|[1.79440486365695...|(18,[0,1,4,15,16,...| 20801.86609984268|-560.9022398426787|\n",
      "| 29814.68354| 19375.07599|20131.08434|      0.0|(12,[1],[1.0])|[1.80604083009823...|(18,[0,1,4,15,16,...|18584.434109315043| 1546.650230684958|\n",
      "| 29128.10127| 19006.68693|19668.43373|      0.0|(12,[1],[1.0])|[1.81022976305639...|(18,[0,1,4,15,16,...|18134.277000510257|1534.1567294897432|\n",
      "| 28228.86076| 18361.09422|18899.27711|      0.0|(12,[1],[1.0])|[1.79866765174088...|(18,[0,1,4,15,16,...|17519.863312803336| 1379.413797196663|\n",
      "|  27335.6962| 17872.34043|18442.40964|      0.0|(12,[1],[1.0])|[1.86328720163797...|(18,[0,1,4,15,16,...|16922.453220235257| 1519.956419764745|\n",
      "+------------+------------+-----------+---------+--------------+--------------------+--------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv_lr_Model.transform(power_spk).withColumn('residual', col('label')-col('prediction')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e679dc6a-31ac-4b06-b977-b8ea469f0cc2",
   "metadata": {},
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a959fc-6abd-4413-a5ac-17a69f59959d",
   "metadata": {},
   "source": [
    "Now that we have a pipeline set up that can transform our data into a shape that we can fit to a model and run predictions, let's apply this model to streaming data. Here, we'll have data in the form of our original data set come in over a time interval (stream). As it comes in, we'll want our model to make predictions, keep a log of the 'label', 'prediction', and the calculated 'residual' and append that log to the transformed data set.\n",
    "\n",
    "To get started, let's import some modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3c27ca5-cbc9-4070-853d-a441bbfec95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import DoubleType, LongType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd15c9f4-ebb7-4dbb-941f-497535042849",
   "metadata": {},
   "source": [
    "Before we set up a ```.readStream()``` instance, we'll need to define the schema of the stream so the ```.readStream()``` instance can define the data types as it comes in. We can review the schema of the data set using ```.printSchema()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acb39b1f-7560-46a2-8bec-2f258ec4b687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Temperature: double (nullable = false)\n",
      " |-- Humidity: double (nullable = false)\n",
      " |-- Wind_Speed: double (nullable = false)\n",
      " |-- General_Diffuse_Flows: double (nullable = false)\n",
      " |-- Diffuse_Flows: double (nullable = false)\n",
      " |-- Power_Zone_1: double (nullable = false)\n",
      " |-- Power_Zone_2: double (nullable = false)\n",
      " |-- Power_Zone_3: double (nullable = false)\n",
      " |-- Month: long (nullable = false)\n",
      " |-- Hour: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "power_spk.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c30398e-7d80-495d-85e6-ffc56c5204a0",
   "metadata": {},
   "source": [
    "What's nice here is that we will be able to define the data types as they come in, so we won't need to do any data type transformations like we did earlier with the 'Hour' column.\n",
    "\n",
    "Once the schema is defined, we can create the stream data frame ('power_df') using ```.readStream()```. Note how we attached the schema with the ```.schema()``` method and indicated that the stream would be in a **csv** format (```.csv()```) in the 'csv_power' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75539f41-e6a1-43a1-951c-8d25b0c89496",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_schema = StructType().add('Temperature', 'double').add('Humidity', 'double').\\\n",
    "add('Wind_Speed', 'double').add('General_Diffuse_Flows', 'double').add('Diffuse_Flows', 'double').\\\n",
    "add('Power_Zone_1', 'double').add('Power_Zone_2', 'double').add('Power_Zone_3', 'double').\\\n",
    "add('Month', 'long').add('Hour', 'double')\n",
    "power_df = spark.readStream.option(\"header\", True).schema(power_schema).csv(\"csv_power\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567b6866-dbcc-4a2d-bd8f-ea9188420425",
   "metadata": {},
   "source": [
    "Since we currently don't have access to a real data stream, we'll have to mimic one. The below code will randomly select three rows from a different power data set. This data was spliced from the original data set that we used in the first two parts, so it is in the same format as the original data set. It will then write the three random rows to a *csv* file located in the 'csv_power' folder. It will repeat this process ten times with a ten second interval betweeen csv files. When this code is executed, it will mimic a live data stream coming in to the 'csv_power' folder.\n",
    "\n",
    "We'll display the code here, but it will actually be executed in a different console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25e8a77d-9ed0-422a-b879-6ef137b7c233",
   "metadata": {},
   "outputs": [],
   "source": [
    "#demo = pd.read_csv(\"power_streaming_data.csv\")\n",
    "#demo_columns = list(demo)\n",
    "\n",
    "#for i in range(0,10):\n",
    "#    #randomly sample a few rows\n",
    "#    temp = demo.loc[np.random.randint(demo.shape[0], size = 3)]\n",
    "#    temp.columns=demo_columns\n",
    "#    temp.to_csv(\"csv_power/demo\" + str(i) + \".csv\", index = False, header = True)\n",
    "#    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499d5ba3-d994-412a-b2b5-46dd80a131d4",
   "metadata": {},
   "source": [
    "Now we need to write out the transform to make the prediction and return the 'label', 'prediction', and 'residual' columns. Notice that we are simply re-using code from the transformations in the previous section, but substituting our stream data frame ('power_df') for the static data frame ('power_spk'). 'power_pred' runs the predictions and creates the residual column. 'pwr_pred' then selects the appropriate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db6aa6e5-27c6-4215-872d-361cf77cd011",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_pred = cv_lr_Model.transform(power_df).withColumn('residual', col('label')-col('prediction'))\n",
    "pwr_pred = power_pred.select(col('label'), col('prediction'), col('residual'))\n",
    "\n",
    "#predQuery = pwr_pred.writeStream.outputMode(\"append\").format(\"console\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed14aa6-a221-43c4-9888-cc3b9f22f3af",
   "metadata": {},
   "source": [
    "'predQuery' was used for testing. We'll execute our entire query when we combine our two transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d30933-438e-46a0-8d4b-6cecb76009c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00ac14c-2ce6-4965-8d8a-f5cd764d65dd",
   "metadata": {},
   "source": [
    "We also want to keep a record of the original stream data as well as all the transformations that have been made. We can do this by simply running the stream through the pipeline but with one important update - we'll need to edit the SQL transformation ('sqlTrans_') to include ALL columns in the SELECT statement. We'll do that below and call it 'sqlTrans_2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d3914c8-ca6c-4158-8542-30c2dd26ee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlTrans_2 = SQLTransformer(statement = '''SELECT Temperature, Humidity, Wind_Speed, General_Diffuse_Flows, \\\n",
    "Diffuse_Flows, Power_Zone_1, Power_Zone_2, Power_Zone_3 as label, Hour, Night_Day, \\\n",
    "Month, Month_OHE, pca FROM __THIS__''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e33133d-d824-40a2-887a-dbed476b0cf2",
   "metadata": {},
   "source": [
    "Now we can update the original pipleline with 'sqlTrans_2' in place of 'sqlTrans_' to create a transformation that will return ALL columns and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "13859d61-7728-41a0-a81b-f575c42fc44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_trans = assembler_2.transform(sqlTrans_2.transform(pca_Trans.transform(assembler_.transform(encoder_Trans.\\\n",
    "                                transform(binary_HTrans.transform(power_df))))))\n",
    "\n",
    "#tran_query = power_trans.writeStream.outputMode('append').format('console').start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33288986-1eb2-4490-a1d1-d1817c6b472b",
   "metadata": {},
   "source": [
    "'tran_query' was for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d34a2536-5606-42c4-ac09-c5a016c8af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tran_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77780f24-7f1a-4385-a581-bc03d180b53e",
   "metadata": {},
   "source": [
    "Now we can join the two stream data frames on the key 'label' so that when we start our stream the output will be a single data frame with all the columns and column transformations as well as the prediction and calculated residual.\n",
    "\n",
    "We'll use ```.writeStream()``` to execute the stream indicating that we want the output to be appended to the console.\n",
    "\n",
    "Once we are done, we can close the session with the ```.stop()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c531d8cb-025b-44bf-94c8-8840934af928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/27 11:13:25 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-1a222d6e-2730-4597-a8e0-a204cdd7a6b5. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/27 11:13:25 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+----+---------+-----+---------------+--------------------+--------------------+------------------+-------------------+\n",
      "|      label|Temperature|Humidity|Wind_Speed|General_Diffuse_Flows|Diffuse_Flows|Power_Zone_1|Power_Zone_2|Hour|Night_Day|Month|      Month_OHE|                 pca|            features|        prediction|           residual|\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+----+---------+-----+---------------+--------------------+--------------------+------------------+-------------------+\n",
      "|    14438.4|      22.03|    74.1|      0.07|                674.9|        566.7| 26625.69536| 12955.50936|10.0|      1.0|    6| (12,[6],[1.0])|[-809.77544782833...|(18,[0,1,2,9,15,1...|11873.968255425854| 2564.4317445741453|\n",
      "|15198.70445|      20.59|    70.8|     0.079|                0.044|        0.167|  24437.5082| 15358.51393| 1.0|      0.0|    5| (12,[5],[1.0])|[1.57501631945422...|(18,[0,1,8,15,16,...|14622.207190598223|  576.4972594017763|\n",
      "|8859.759036|      19.45|   58.64|     0.081|                113.4|         81.9| 26603.07692| 21488.42975| 9.0|      1.0|   11|(12,[11],[1.0])|[-131.07739108138...|(18,[0,1,2,14,15,...| 10891.29993786625|-2031.5409018662504|\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+----+---------+-----+---------------+--------------------+--------------------+------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+----+---------+-----+---------------+--------------------+--------------------+------------------+-------------------+\n",
      "|      label|Temperature|Humidity|Wind_Speed|General_Diffuse_Flows|Diffuse_Flows|Power_Zone_1|Power_Zone_2|Hour|Night_Day|Month|      Month_OHE|                 pca|            features|        prediction|           residual|\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+----+---------+-----+---------------+--------------------+--------------------+------------------+-------------------+\n",
      "|19106.62614|      21.17|    65.7|     4.924|                36.66|        26.44| 45903.54486| 29124.89627|18.0|      1.0|   10|(12,[10],[1.0])|[-41.281178786024...|(18,[0,1,2,13,15,...| 21764.21054636958|-2657.5844063695804|\n",
      "|17563.37349|      22.25|   51.47|     0.073|                379.4|         93.1| 32886.15385| 24061.98347|15.0|      1.0|   11|(12,[11],[1.0])|[-388.69930611383...|(18,[0,1,2,14,15,...|14318.689502085716| 3244.6839879142863|\n",
      "|12348.69301|      19.56|    88.9|     0.082|                174.2|        148.3| 35631.33479| 25248.54772|13.0|      1.0|   10|(12,[10],[1.0])|[-207.90685217978...|(18,[0,1,2,13,15,...|14125.476923522736| -1776.783913522735|\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+----+---------+-----+---------------+--------------------+--------------------+------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+----+---------+-----+--------------+--------------------+--------------------+------------------+-------------------+\n",
      "|      label|Temperature|Humidity|Wind_Speed|General_Diffuse_Flows|Diffuse_Flows|Power_Zone_1|Power_Zone_2|Hour|Night_Day|Month|     Month_OHE|                 pca|            features|        prediction|           residual|\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+----+---------+-----+--------------+--------------------+--------------------+------------------+-------------------+\n",
      "|16372.36364|      15.36|   67.42|     0.075|                0.026|        0.193| 23511.21636| 12940.93686| 2.0|      0.0|    4|(12,[4],[1.0])|[1.54529564709417...|(18,[0,1,7,15,16,...|15513.622925345942|  858.7407146540572|\n",
      "|12044.69636|       18.2|    87.2|     4.916|                0.055|        0.096| 21441.04918| 13408.04954| 5.0|      0.0|    5|(12,[5],[1.0])|[2.03425390488008...|(18,[0,1,8,15,16,...|12365.405848051472|-320.70948805147236|\n",
      "|26986.33846|      20.65|    85.4|     0.066|                0.048|        0.141| 43765.82781| 25409.56341|23.0|      1.0|    6|(12,[6],[1.0])|[1.96217261627415...|(18,[0,1,2,9,15,1...| 26671.03628500013| 315.30217499986975|\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+----+---------+-----+--------------+--------------------+--------------------+------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+----+---------+-----+--------------+--------------------+--------------------+------------------+-------------------+\n",
      "|      label|Temperature|Humidity|Wind_Speed|General_Diffuse_Flows|Diffuse_Flows|Power_Zone_1|Power_Zone_2|Hour|Night_Day|Month|     Month_OHE|                 pca|            features|        prediction|           residual|\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+----+---------+-----+--------------+--------------------+--------------------+------------------+-------------------+\n",
      "|38500.41841|       25.5|    79.7|     4.914|                0.084|        0.152| 41302.32558| 29001.26582|22.0|      1.0|    7|(12,[7],[1.0])|[1.72363695419787...|(18,[0,1,2,10,15,...|32106.238660672734|  6394.179749327264|\n",
      "|12055.27273|      14.09|    89.3|     0.067|                 0.07|        0.108| 23672.42196| 14304.68432| 6.0|      0.0|    4|(12,[4],[1.0])|[2.11562721136380...|(18,[0,1,7,15,16,...|15410.095999298224|-3354.8232692982237|\n",
      "|22738.70769|      20.99|    89.4|     4.917|                0.069|        0.145| 29581.98675| 18325.57173| 1.0|      0.0|    6|(12,[6],[1.0])|[2.03784188251680...|(18,[0,1,9,15,16,...| 19101.95905008695| 3636.7486399130503|\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+----+---------+-----+--------------+--------------------+--------------------+------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+----+---------+-----+---------------+--------------------+--------------------+------------------+------------------+\n",
      "|      label|Temperature|Humidity|Wind_Speed|General_Diffuse_Flows|Diffuse_Flows|Power_Zone_1|Power_Zone_2|Hour|Night_Day|Month|      Month_OHE|                 pca|            features|        prediction|          residual|\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+----+---------+-----+---------------+--------------------+--------------------+------------------+------------------+\n",
      "|14781.68675|      10.19|    75.8|     0.087|                0.084|        0.137| 23471.39241| 14392.70517| 2.0|      0.0|    1| (12,[1],[1.0])|[1.77572739499498...|(18,[0,1,4,15,16,...| 14251.34925744154| 530.3374925584612|\n",
      "|26318.76923|      20.45|    82.6|     0.071|                0.062|        0.115| 45062.78146| 24578.79418|22.0|      1.0|    6| (12,[6],[1.0])|[1.88472080674342...|(18,[0,1,2,9,15,1...| 27405.53456560815| -1086.76533560815|\n",
      "|11035.56923|      19.12|   65.52|     0.077|                29.83|        18.76| 21514.17219| 10306.02911| 6.0|      0.0|    6| (12,[6],[1.0])|[-32.476476654189...|(18,[0,1,9,15,16,...|13665.404669912128|-2629.835439912129|\n",
      "| 13008.1459|      19.68|    84.9|     0.069|                0.059|        0.096| 28024.85777| 22029.46058|23.0|      1.0|   10|(12,[10],[1.0])|[1.96095784019033...|(18,[0,1,2,13,15,...|10177.151171722322|2830.9947282776775|\n",
      "|10565.78313|      13.62|    81.7|     0.083|                4.191|        3.976| 18873.84615|  12689.2562| 7.0|      1.0|   11|(12,[11],[1.0])|[-3.1549228830191...|(18,[0,1,2,14,15,...| 5484.841312314908| 5080.941817685092|\n",
      "|11479.27273|      11.59|    79.6|     0.083|                0.037|        0.189| 20894.72551| 12350.71283| 5.0|      0.0|    4| (12,[4],[1.0])|[1.89196345979987...|(18,[0,1,7,15,16,...|13686.803725536593|-2207.530995536592|\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+----+---------+-----+---------------+--------------------+--------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+----+---------+-----+--------------+--------------------+--------------------+------------------+------------------+\n",
      "|      label|Temperature|Humidity|Wind_Speed|General_Diffuse_Flows|Diffuse_Flows|Power_Zone_1|Power_Zone_2|Hour|Night_Day|Month|     Month_OHE|                 pca|            features|        prediction|          residual|\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+----+---------+-----+--------------+--------------------+--------------------+------------------+------------------+\n",
      "|9865.066026|      13.39|    70.8|      0.08|                0.037|        0.134| 23726.23574| 19301.62627| 1.0|      0.0|   12|    (12,[],[])|[1.65970102172591...|(18,[0,1,15,16,17...| 8990.273810711387| 874.7922152886131|\n",
      "|22799.24765|      26.84|   66.49|     4.961|                789.0|         72.3| 39424.99445|     25200.0|14.0|      1.0|    8|(12,[8],[1.0])|[-773.52374371160...|(18,[0,1,2,11,15,...|24542.530573222335|-1743.282923222334|\n",
      "|28197.48954|      36.84|   20.88|     4.915|                794.0|         70.6| 39293.02326| 27129.11392|15.0|      1.0|    7|(12,[7],[1.0])|[-779.09397694355...|(18,[0,1,2,10,15,...|29373.443291054213|-1175.953751054214|\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+----+---------+-----+--------------+--------------------+--------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+----+---------+-----+---------------+--------------------+--------------------+------------------+-------------------+\n",
      "|      label|Temperature|Humidity|Wind_Speed|General_Diffuse_Flows|Diffuse_Flows|Power_Zone_1|Power_Zone_2|Hour|Night_Day|Month|      Month_OHE|                 pca|            features|        prediction|           residual|\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+----+---------+-----+---------------+--------------------+--------------------+------------------+-------------------+\n",
      "| 12017.3494|       15.6|    85.3|      0.07|                0.037|        0.163| 24923.07692|  19033.8843| 1.0|      0.0|   11|(12,[11],[1.0])|[2.01146835606787...|(18,[0,1,14,15,16...|11471.745773230632|  545.6036267693671|\n",
      "|12717.10843|      12.15|    85.5|     0.067|                0.048|        0.145| 22609.23077| 16185.12397| 5.0|      0.0|   11|(12,[11],[1.0])|[2.04423990641043...|(18,[0,1,14,15,16...| 9778.438950478645| 2938.6694795213552|\n",
      "|19003.17992|      24.08|   60.36|     4.903|                0.102|         0.13| 24577.27575| 16556.96203| 5.0|      0.0|    7| (12,[7],[1.0])|[1.21788405156349...|(18,[0,1,10,15,16...|22813.988340053016|-3810.8084200530175|\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+----+---------+-----+---------------+--------------------+--------------------+------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+----+---------+-----+---------------+--------------------+--------------------+------------------+-------------------+\n",
      "|      label|Temperature|Humidity|Wind_Speed|General_Diffuse_Flows|Diffuse_Flows|Power_Zone_1|Power_Zone_2|Hour|Night_Day|Month|      Month_OHE|                 pca|            features|        prediction|           residual|\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+----+---------+-----+---------------+--------------------+--------------------+------------------+-------------------+\n",
      "|20145.23077|      28.59|   29.65|     4.919|                748.0|        43.24| 37929.53642| 23272.76507|15.0|      1.0|    6| (12,[6],[1.0])|[-726.78910829306...|(18,[0,1,2,9,15,1...| 21761.95956295512|-1616.7287929551203|\n",
      "|10878.07229|      20.82|   63.12|     4.919|                227.1|        201.3| 27126.15385| 21019.83471|16.0|      1.0|   11|(12,[11],[1.0])|[-274.73518364307...|(18,[0,1,2,14,15,...|10447.641316045116|  430.4309739548844|\n",
      "|20329.06883|      16.42|    88.3|     4.921|                0.059|        0.122| 34194.88525| 20920.12384| 0.0|      0.0|    5| (12,[5],[1.0])|[2.06858020166900...|(18,[0,1,8,15,16,...|20771.654847206006|-442.58601720600564|\n",
      "|7957.743097|      6.184|    88.6|     0.082|                55.82|        26.63|  24158.1749| 19607.24149| 8.0|      1.0|   12|     (12,[],[])|[-58.891429391890...|(18,[0,1,2,15,16,...| 7036.868840799565|  920.8742562004345|\n",
      "|27845.81818|      15.29|    85.0|     0.075|                0.029|        0.133| 41826.65231| 21574.33809|20.0|      1.0|    4| (12,[4],[1.0])|[2.02298677853759...|(18,[0,1,2,7,15,1...|  25388.3802041127|    2457.4379758873|\n",
      "| 9050.60241|      17.98|   62.75|      4.92|                257.3|        49.77| 31046.15385|     24750.0|10.0|      1.0|   11|(12,[11],[1.0])|[-258.97264921218...|(18,[0,1,2,14,15,...|13548.111035984955| -4497.508625984956|\n",
      "+-----------+-----------+--------+----------+---------------------+-------------+------------+------------+----+---------+-----+---------------+--------------------+--------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_power = power_trans.join(pwr_pred, 'label', 'inner')\\\n",
    ".writeStream.outputMode('append').format('console').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a39c00a2-2d09-43ba-bff5-0c460adffdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_power.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701f175a-bbe3-463c-9ed7-cd77d122c460",
   "metadata": {},
   "source": [
    "That's it! You can view the execution of the code in the attached screen video."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
